{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.8.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and installs\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import first\n",
    "from pyspark.sql.functions import upper, col\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType\n",
    "from pyspark.sql.functions import udf, date_format\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore and assess the data\n",
    "\n",
    "## Explore data\n",
    "# - Identify data quality issues (missing values, duplicate data etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read us-cities-demographics.csv\n",
    "us_spark = spark.read.csv(\"./datasets/us-cities-demographics.csv\", sep=\";\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['City',\n",
       " 'State',\n",
       " 'Median Age',\n",
       " 'Male Population',\n",
       " 'Female Population',\n",
       " 'Total Population',\n",
       " 'Number of Veterans',\n",
       " 'Foreign-born',\n",
       " 'Average Household Size',\n",
       " 'State Code',\n",
       " 'Race',\n",
       " 'Count']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Check columns of the dataset\n",
    "us_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+----------+----------+---------------+-----------------+----------------+------------+----------------------+\n|   City|     State|Median Age|Male Population|Female Population|Total Population|Foreign-born|Average Household Size|\n+-------+----------+----------+---------------+-----------------+----------------+------------+----------------------+\n|Abilene|     Texas|      31.3|          65212|            60664|          125876|        8129|                  2.64|\n|Abilene|     Texas|      31.3|          65212|            60664|          125876|        8129|                  2.64|\n|Abilene|     Texas|      31.3|          65212|            60664|          125876|        8129|                  2.64|\n|Abilene|     Texas|      31.3|          65212|            60664|          125876|        8129|                  2.64|\n|Abilene|     Texas|      31.3|          65212|            60664|          125876|        8129|                  2.64|\n|  Akron|      Ohio|      38.1|          96886|           100667|          197553|       10024|                  2.24|\n|  Akron|      Ohio|      38.1|          96886|           100667|          197553|       10024|                  2.24|\n|  Akron|      Ohio|      38.1|          96886|           100667|          197553|       10024|                  2.24|\n|  Akron|      Ohio|      38.1|          96886|           100667|          197553|       10024|                  2.24|\n|  Akron|      Ohio|      38.1|          96886|           100667|          197553|       10024|                  2.24|\n|Alafaya|   Florida|      33.5|          39504|            45760|           85264|       15842|                  2.94|\n|Alafaya|   Florida|      33.5|          39504|            45760|           85264|       15842|                  2.94|\n|Alafaya|   Florida|      33.5|          39504|            45760|           85264|       15842|                  2.94|\n|Alafaya|   Florida|      33.5|          39504|            45760|           85264|       15842|                  2.94|\n|Alameda|California|      41.4|          37747|            40867|           78614|       18841|                  2.52|\n|Alameda|California|      41.4|          37747|            40867|           78614|       18841|                  2.52|\n|Alameda|California|      41.4|          37747|            40867|           78614|       18841|                  2.52|\n|Alameda|California|      41.4|          37747|            40867|           78614|       18841|                  2.52|\n|Alameda|California|      41.4|          37747|            40867|           78614|       18841|                  2.52|\n| Albany|  New York|      32.8|          47627|            50825|           98452|       11948|                  2.08|\n+-------+----------+----------+---------------+-----------------+----------------+------------+----------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Check us_spark dataset for repeated rows, and which columns cause the duplicates\n",
    "us_spark.select(\"City\", \"State\", \"Median Age\", \"Male Population\", \"Female Population\", \"Total Population\", \\\n",
    "                \"Foreign-born\", \"Average Household Size\").orderBy(\"City\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+----------+--------------------+------+\n|   City|State Code|                Race| Count|\n+-------+----------+--------------------+------+\n|Abilene|        TX|American Indian a...|  1813|\n|Abilene|        TX|  Hispanic or Latino| 33222|\n|Abilene|        TX|               White| 95487|\n|Abilene|        TX|               Asian|  2929|\n|Abilene|        TX|Black or African-...| 14449|\n|  Akron|        OH|               White|129192|\n|  Akron|        OH|  Hispanic or Latino|  3684|\n|  Akron|        OH|Black or African-...| 66551|\n|  Akron|        OH|               Asian|  9033|\n|  Akron|        OH|American Indian a...|  1845|\n|Alafaya|        FL|  Hispanic or Latino| 34897|\n|Alafaya|        FL|               Asian| 10336|\n|Alafaya|        FL|               White| 63666|\n|Alafaya|        FL|Black or African-...|  6577|\n|Alameda|        CA|               White| 44232|\n|Alameda|        CA|American Indian a...|  1329|\n|Alameda|        CA|Black or African-...|  7364|\n|Alameda|        CA|  Hispanic or Latino|  8265|\n|Alameda|        CA|               Asian| 27984|\n| Albany|        NY|  Hispanic or Latino|  9368|\n+-------+----------+--------------------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Check subset of `US` dataset that maybe causing dupliate rows\n",
    "us_spark.select(\"City\",\"State Code\",\"Race\",\"Count\").orderBy(\"City\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n|   City|     State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race| Count|\n+-------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n|Abilene|     Texas|      31.3|          65212|            60664|          125876|              9367|        8129|                  2.64|        TX|American Indian a...|  1813|\n|Abilene|     Texas|      31.3|          65212|            60664|          125876|              9367|        8129|                  2.64|        TX|  Hispanic or Latino| 33222|\n|Abilene|     Texas|      31.3|          65212|            60664|          125876|              9367|        8129|                  2.64|        TX|               White| 95487|\n|Abilene|     Texas|      31.3|          65212|            60664|          125876|              9367|        8129|                  2.64|        TX|               Asian|  2929|\n|Abilene|     Texas|      31.3|          65212|            60664|          125876|              9367|        8129|                  2.64|        TX|Black or African-...| 14449|\n|  Akron|      Ohio|      38.1|          96886|           100667|          197553|             12878|       10024|                  2.24|        OH|               White|129192|\n|  Akron|      Ohio|      38.1|          96886|           100667|          197553|             12878|       10024|                  2.24|        OH|  Hispanic or Latino|  3684|\n|  Akron|      Ohio|      38.1|          96886|           100667|          197553|             12878|       10024|                  2.24|        OH|Black or African-...| 66551|\n|  Akron|      Ohio|      38.1|          96886|           100667|          197553|             12878|       10024|                  2.24|        OH|               Asian|  9033|\n|  Akron|      Ohio|      38.1|          96886|           100667|          197553|             12878|       10024|                  2.24|        OH|American Indian a...|  1845|\n|Alafaya|   Florida|      33.5|          39504|            45760|           85264|              4176|       15842|                  2.94|        FL|  Hispanic or Latino| 34897|\n|Alafaya|   Florida|      33.5|          39504|            45760|           85264|              4176|       15842|                  2.94|        FL|               Asian| 10336|\n|Alafaya|   Florida|      33.5|          39504|            45760|           85264|              4176|       15842|                  2.94|        FL|               White| 63666|\n|Alafaya|   Florida|      33.5|          39504|            45760|           85264|              4176|       15842|                  2.94|        FL|Black or African-...|  6577|\n|Alameda|California|      41.4|          37747|            40867|           78614|              4504|       18841|                  2.52|        CA|               White| 44232|\n|Alameda|California|      41.4|          37747|            40867|           78614|              4504|       18841|                  2.52|        CA|American Indian a...|  1329|\n|Alameda|California|      41.4|          37747|            40867|           78614|              4504|       18841|                  2.52|        CA|Black or African-...|  7364|\n|Alameda|California|      41.4|          37747|            40867|           78614|              4504|       18841|                  2.52|        CA|  Hispanic or Latino|  8265|\n|Alameda|California|      41.4|          37747|            40867|           78614|              4504|       18841|                  2.52|        CA|               Asian| 27984|\n| Albany|  New York|      32.8|          47627|            50825|           98452|              3643|       11948|                  2.08|        NY|  Hispanic or Latino|  9368|\n+-------+----------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "## Printing all columns to check again\n",
    "us_spark.select(\"City\", \"State\", \"Median Age\", \"Male Population\", \"Female Population\", \"Total Population\", \\\n",
    "                \"Number of Veterans\", \"Foreign-born\", \"Average Household Size\", \"State Code\", \\\n",
    "                \"Race\", \"Count\").orderBy(\"City\").show()\n",
    "\n",
    "#  ['City',\n",
    "#  'State',\n",
    "#  'Median Age',\n",
    "#  'Male Population',\n",
    "#  'Female Population',\n",
    "#  'Total Population',\n",
    "#  'Number of Veterans',\n",
    "#  'Foreign-born',\n",
    "#  'Average Household Size',\n",
    "#  'State Code',\n",
    "#  'Race',\n",
    "#  'Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the dataset --- Approach:\n",
    "## - 'Race' and 'Count' columns are the cause of duplicate rows. Hence, the approach is to separate them into their own dataset, and include \"City\" and \"State Code\" columns for reference\n",
    "## - Cleaned-up US dataset can be joined back into us_race_cnt dataset to, eventually, contain unique rows\n",
    "us_race_cnt = (us_spark.select(\"City\", \"State Code\", \"Race\", \"Count\")\n",
    "            .groupBy(us_spark.City, \"State Code\")\n",
    "            .pivot(\"Race\")\n",
    "            .agg(first(\"Count\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------+----------+---------------------------------+-----+-------------------------+------------------+------+\n|        City|State Code|American Indian and Alaska Native|Asian|Black or African-American|Hispanic or Latino| White|\n+------------+----------+---------------------------------+-----+-------------------------+------------------+------+\n|     Abilene|        TX|                             1813| 2929|                    14449|             33222| 95487|\n|       Akron|        OH|                             1845| 9033|                    66551|              3684|129192|\n|     Alafaya|        FL|                             null|10336|                     6577|             34897| 63666|\n|     Alameda|        CA|                             1329|27984|                     7364|              8265| 44232|\n|      Albany|        GA|                              445|  650|                    53440|              1783| 17160|\n|      Albany|        NY|                             1611| 8090|                    31303|              9368| 58368|\n| Albuquerque|        NM|                            32243|25140|                    26774|            271854|411847|\n|  Alexandria|        VA|                             1133|13315|                    37168|             25573|106215|\n|    Alhambra|        CA|                              687|44067|                     1905|             31386| 20811|\n|       Allen|        TX|                              227|15790|                    13140|             10615| 69840|\n|       Allen|        PA|                             1076| 2670|                    22304|             59176| 74187|\n|    Amarillo|        TX|                             4260| 8563|                    14050|             65392|174214|\n|        Ames|        IA|                             null| 8979|                     1103|              2024| 56157|\n|     Anaheim|        CA|                             2489|53270|                     9775|            201593|259820|\n|   Anchorage|        AK|                            36339|36825|                    23107|             27261|212696|\n|   Ann Arbor|        MI|                             1935|18797|                     9577|              5888| 90173|\n|     Antioch|        CA|                             3462|14333|                    23227|             35563| 51151|\n|Apple Valley|        CA|                             1446| 2281|                     9124|             25928| 60767|\n|    Appleton|        WI|                              835| 5561|                     3407|              5139| 64674|\n|Arden-Arcade|        CA|                             2587| 7355|                    13647|             15273| 69369|\n+------------+----------+---------------------------------+-----+-------------------------+------------------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "## Check us_race_cnt dataset\n",
    "us_race_cnt.orderBy(\"City\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(596, 596)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Compare datasets - before and after dropping duplicate rows\n",
    "(us_race_cnt.count(), us_race_cnt.dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "uscols=[\"Number of Veterans\", \"Race\", \"Count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "us=us_spark.drop(*uscols).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2891, 596)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Compare row count between original and new dataset with dropped duplicate rows\n",
    "# the cleaned 'us' dataset matches number of rows with 'us_race_cnt' dataset, which will be combined into one 'us' dataset\n",
    "(us_spark.count(), us.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Check number of records after combining both data sets\n",
    "# Total rows should match both datasets\n",
    "us.join(us_race_cnt, [\"City\", \"State Code\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------+----------+------------+----------+---------------+-----------------+----------------+------------+----------------------+---------------------------------+-----+-------------------------+------------------+------+\n|        City|State Code|       State|Median Age|Male Population|Female Population|Total Population|Foreign-born|Average Household Size|American Indian and Alaska Native|Asian|Black or African-American|Hispanic or Latino| White|\n+------------+----------+------------+----------+---------------+-----------------+----------------+------------+----------------------+---------------------------------+-----+-------------------------+------------------+------+\n|     Abilene|        TX|       Texas|      31.3|          65212|            60664|          125876|        8129|                  2.64|                             1813| 2929|                    14449|             33222| 95487|\n|       Akron|        OH|        Ohio|      38.1|          96886|           100667|          197553|       10024|                  2.24|                             1845| 9033|                    66551|              3684|129192|\n|     Alafaya|        FL|     Florida|      33.5|          39504|            45760|           85264|       15842|                  2.94|                             null|10336|                     6577|             34897| 63666|\n|     Alameda|        CA|  California|      41.4|          37747|            40867|           78614|       18841|                  2.52|                             1329|27984|                     7364|              8265| 44232|\n|      Albany|        NY|    New York|      32.8|          47627|            50825|           98452|       11948|                  2.08|                             1611| 8090|                    31303|              9368| 58368|\n|      Albany|        GA|     Georgia|      33.3|          31695|            39414|           71109|         861|                  2.38|                              445|  650|                    53440|              1783| 17160|\n| Albuquerque|        NM|  New Mexico|      36.0|         273323|           285808|          559131|       58200|                  2.49|                            32243|25140|                    26774|            271854|411847|\n|  Alexandria|        VA|    Virginia|      36.6|          74989|            78522|          153511|       44030|                   2.2|                             1133|13315|                    37168|             25573|106215|\n|    Alhambra|        CA|  California|      41.0|          42184|            43388|           85572|       44441|                  2.89|                              687|44067|                     1905|             31386| 20811|\n|       Allen|        TX|       Texas|      37.2|          51324|            46814|           98138|       19649|                  3.04|                              227|15790|                    13140|             10615| 69840|\n|       Allen|        PA|Pennsylvania|      33.5|          60626|            59581|          120207|       19652|                  2.67|                             1076| 2670|                    22304|             59176| 74187|\n|    Amarillo|        TX|       Texas|      33.8|          99391|           100260|          199651|       21124|                  2.64|                             4260| 8563|                    14050|             65392|174214|\n|        Ames|        IA|        Iowa|      23.0|          33814|            31238|           65052|        8606|                  2.16|                             null| 8979|                     1103|              2024| 56157|\n|     Anaheim|        CA|  California|      33.6|         179603|           171135|          350738|      137133|                  3.45|                             2489|53270|                     9775|            201593|259820|\n|   Anchorage|        AK|      Alaska|      32.2|         152945|           145750|          298695|       33258|                  2.77|                            36339|36825|                    23107|             27261|212696|\n|   Ann Arbor|        MI|    Michigan|      28.1|          58789|            58281|          117070|       20717|                  2.17|                             1935|18797|                     9577|              5888| 90173|\n|     Antioch|        CA|  California|      34.0|          54733|            55809|          110542|       24942|                  3.31|                             3462|14333|                    23227|             35563| 51151|\n|Apple Valley|        CA|  California|      34.3|          32873|            39312|           72185|        5801|                  3.03|                             1446| 2281|                     9124|             25928| 60767|\n|    Appleton|        WI|   Wisconsin|      35.6|          37217|            38038|           75255|        4454|                  2.49|                              835| 5561|                     3407|              5139| 64674|\n|Arden-Arcade|        CA|  California|      41.5|          47596|            48680|           96276|       13458|                  2.18|                             2587| 7355|                    13647|             15273| 69369|\n+------------+----------+------------+----------+---------------+-----------------+----------------+------------+----------------------+---------------------------------+-----+-------------------------+------------------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# check the data sample\n",
    "us.join(us_race_cnt, [\"City\", \"State Code\"]).orderBy(\"city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running the above check, we commit to the combination of the 'us' dataset\n",
    "us=us.join(us_race_cnt, [\"City\", \"State Code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------+----------+-----------+----------+---------------+-----------------+----------------+------------+----------------------+---------------------------------+-----+-------------------------+------------------+------+\n|           City|State Code|      State|Median Age|Male Population|Female Population|Total Population|Foreign-born|Average Household Size|American Indian and Alaska Native|Asian|Black or African-American|Hispanic or Latino| White|\n+---------------+----------+-----------+----------+---------------+-----------------+----------------+------------+----------------------+---------------------------------+-----+-------------------------+------------------+------+\n|Highlands Ranch|        CO|   Colorado|      39.6|          49186|            53281|          102467|        8827|                  2.72|                             1480| 5650|                     1779|              8393| 94499|\n|           Kent|        WA| Washington|      33.4|          61825|            65137|          126962|       38175|                  3.06|                             3651|26168|                    20450|             21928| 67918|\n|        Madison|        WI|  Wisconsin|      30.7|         122596|           126360|          248956|       30090|                  2.23|                             2296|23937|                    20424|             19697|204302|\n|         Denver|        CO|   Colorado|      34.1|         341137|           341408|          682545|      113222|                  2.33|                            14008|32491|                    72288|            207847|546370|\n|         Caguas|        PR|Puerto Rico|      40.4|          34743|            42265|           77008|        null|                  null|                              624| null|                     null|             76349|  null|\n+---------------+----------+-----------+----------+---------------+-----------------+----------------+------------+----------------------+---------------------------------+-----+-------------------------+------------------+------+\nonly showing top 5 rows\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(596, None)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "#Secondary check\n",
    "(us.count(), us.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change `State Code` column name to `state_code` and other similar problems to avoid parquet complications (remove whitespaces)\n",
    "us=us.select('City', col('State Code').alias('State_Code'), 'State', col('Median Age').alias('Median_age'),\n",
    "     col('Male Population').alias('Male_Pop'), col('Female Population').alias('Fem_Pop'), \n",
    "        col('Total Population').alias('Ttl_Pop'), 'Foreign-born', \n",
    "          col('Average Household Size').alias('Avg_Household_Size'),\n",
    "             col('American Indian and Alaska Native').alias('Native_Pop'), \n",
    "                 col('Asian').alias('Asian_Pop'), \n",
    "                    col('Black or African-American').alias('Black_Pop'), \n",
    "                      col('Hispanic or Latino').alias('Latino_Pop'), \n",
    "                        col('White').alias('White_Pop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------+----------+----------+----------+--------+-------+-------+------------+------------------+----------+---------+---------+----------+---------+\n|           City|State_Code|     State|Median_age|Male_Pop|Fem_Pop|Ttl_Pop|Foreign-born|Avg_Household_Size|Native_Pop|Asian_Pop|Black_Pop|Latino_Pop|White_Pop|\n+---------------+----------+----------+----------+--------+-------+-------+------------+------------------+----------+---------+---------+----------+---------+\n|Highlands Ranch|        CO|  Colorado|      39.6|   49186|  53281| 102467|        8827|              2.72|      1480|     5650|     1779|      8393|    94499|\n|           Kent|        WA|Washington|      33.4|   61825|  65137| 126962|       38175|              3.06|      3651|    26168|    20450|     21928|    67918|\n+---------------+----------+----------+----------+--------+-------+-------+------------+------------------+----------+---------+---------+----------+---------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Check the new column names\n",
    "us.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------+----------+----------+--------+-------+-------+------------+------------------+----------+---------+---------+----------+---------+\n|           City|State_Code|Median_age|Male_Pop|Fem_Pop|Ttl_Pop|Foreign-born|Avg_Household_Size|Native_Pop|Asian_Pop|Black_Pop|Latino_Pop|White_Pop|\n+---------------+----------+----------+--------+-------+-------+------------+------------------+----------+---------+---------+----------+---------+\n|Highlands Ranch|        CO|      39.6|   49186|  53281| 102467|        8827|              2.72|      1480|     5650|     1779|      8393|    94499|\n|           Kent|        WA|      33.4|   61825|  65137| 126962|       38175|              3.06|      3651|    26168|    20450|     21928|    67918|\n+---------------+----------+----------+--------+-------+-------+------------+------------------+----------+---------+---------+----------+---------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Drop 'state' column\n",
    "us=us.drop(\"State\")\n",
    "us.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write transformed 'us' dataset onto parquet file\n",
    "us.write.mode('overwrite').parquet(\"./datasets/us_cities_demographics.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "## I94 Non-Immigration Dataset (Cleaning and Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read i94 non-immigration dataset\n",
    "i94_spark = spark.read.parquet(\"./datasets/sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['cicid',\n",
       " 'i94yr',\n",
       " 'i94mon',\n",
       " 'i94cit',\n",
       " 'i94res',\n",
       " 'i94port',\n",
       " 'arrdate',\n",
       " 'i94mode',\n",
       " 'i94addr',\n",
       " 'depdate',\n",
       " 'i94bir',\n",
       " 'i94visa',\n",
       " 'count',\n",
       " 'dtadfile',\n",
       " 'visapost',\n",
       " 'occup',\n",
       " 'entdepa',\n",
       " 'entdepd',\n",
       " 'entdepu',\n",
       " 'matflag',\n",
       " 'biryear',\n",
       " 'dtaddto',\n",
       " 'gender',\n",
       " 'insnum',\n",
       " 'airline',\n",
       " 'admnum',\n",
       " 'fltno',\n",
       " 'visatype']"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "i94_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n|i94res|i94port|arrdate|i94mode|depdate|i94bir|i94visa|count|gender|     admnum|\n+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n| 438.0|    LOS|20574.0|    1.0|20582.0|  40.0|    1.0|  1.0|     F|94953870030|\n| 438.0|    LOS|20574.0|    1.0|20591.0|  32.0|    1.0|  1.0|     F|94955622830|\n| 438.0|    LOS|20574.0|    1.0|20582.0|  29.0|    1.0|  1.0|     M|94956406530|\n+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# show a sample of `sas_data`\n",
    "i94_spark.select(\"i94res\",\"i94port\",\"arrdate\",\"i94mode\",\"depdate\",\"i94bir\",\"i94visa\",\"count\" \\\n",
    "                  ,\"gender\",col(\"admnum\").cast(LongType())).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n|i94res|i94port|arrdate|i94mode|depdate|i94bir|i94visa|count|gender|     admnum|\n+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n|   438|    LOS|  20574|      1|  20582|    40|      1|    1|     F|94953870030|\n|   438|    LOS|  20574|      1|  20591|    32|      1|    1|     F|94955622830|\n|   438|    LOS|  20574|      1|  20582|    29|      1|    1|     M|94956406530|\n+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Type Conversion\n",
    "i94_spark = i94_spark.select(col(\"i94res\").cast(IntegerType()), \n",
    "                             col(\"i94port\"),\n",
    "                             col(\"arrdate\").cast(IntegerType()),\n",
    "                             col(\"i94mode\").cast(IntegerType()),\n",
    "                             col(\"depdate\").cast(IntegerType()),\n",
    "                             col(\"i94bir\").cast(IntegerType()),\n",
    "                             col(\"i94visa\").cast(IntegerType()), \n",
    "                             col(\"count\").cast(IntegerType()), \\\n",
    "                             \"gender\",\n",
    "                             col(\"admnum\").cast(LongType()))\n",
    "\n",
    "# Show top 3 rows\n",
    "i94_spark.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3096313, 3096302)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "## Check for duplicate rows on each dataset by comparing original total rows\n",
    "i94_spark.count(), i94_spark.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3075579"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "i94_spark.dropDuplicates(['admnum']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From the above 2 commands, 'admnum' may not be unique as assumed. \n",
    "## We should drop duplicate rows instead of dropping duplicates only by admnum so we will have unique rows\n",
    "i94_spark=i94_spark.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n|i94res|i94port|arrdate|i94mode|depdate|i94bir|i94visa|count|gender|     admnum|\n+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\n|   209|    AGA|  20552|      1|   null|  null|      2|    1|     M|47842155333|\n|   209|    ATL|  20571|      1|   null|  null|      2|    1|     M|44537883633|\n|   696|    FTL|  20574|      1|  20597|     0|      2|    1|     M|95022828130|\n+------+-------+-------+-------+-------+------+-------+-----+------+-----------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "i94_spark.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll save i94_spark dataframe to parquet file after joining i94port and us dataframes later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning of i94 code lists and creating dimension tables\n",
    "## I94_SAS_Labels_Descriptions.SAS file was used to create master i94 code lists by creating text files and then cleaning whitespaces and quotations; converting them into python lists, then saving them as parquet files. These files are used as master record attribute files as they do not change often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin processing I94_SAS_Labels_Descriptions.SAS to create master i94 code dimensions\n",
    "'''\n",
    "/* I94MODE - missing values as well as not reported*/\n",
    "\t1 = 'Air'\n",
    "\t2 = 'Sea'\n",
    "\t3 = 'Land'\n",
    "\t9 = 'Not reported' ;\n",
    "'''\n",
    "i94mode_data = [[1,'Air'],[2,'Sea'],[3,'Land'],[9,'Not reported']]\n",
    "\n",
    "# Convert to spark dataframe\n",
    "i94mode = spark.createDataFrame(i94mode_data)\n",
    "\n",
    "# Create i94mode parquest file\n",
    "i94mode.write.mode(\"overwrite\").parquet('./data/i94mode.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-34-8b0f39a8dd25>:10: FutureWarning: Columnar iteration over characters will be deprecated in future releases.\n  i94port_df['port_city'], i94port_df['port_state']=i94port_df['port'].str.strip().str.replace(\"'\",'').str.strip().str.split(',',1).str\n"
     ]
    }
   ],
   "source": [
    "# Create i94port.txt file by copying from I94_SAS_Labels_Descriptions.SAS\n",
    "# Read i94port text file\n",
    "i94port_df = pd.read_csv('./data/i94port.txt',sep='=',names=['id','port'])\n",
    "\n",
    "# Remove whitespaces and single quotes\n",
    "i94port_df['id']=i94port_df['id'].str.strip().str.replace(\"'\",'')\n",
    "\n",
    "# Create two columns from i94port string: port_city and port_addr\n",
    "# also remove whitespaces and single quotes\n",
    "i94port_df['port_city'], i94port_df['port_state']=i94port_df['port'].str.strip().str.replace(\"'\",'').str.strip().str.split(',',1).str\n",
    "\n",
    "# Remove more whitespace from port_addr\n",
    "i94port_df['port_state']=i94port_df['port_state'].str.strip()\n",
    "\n",
    "# Drop port column and keep the two new columns: port_city and port_addr\n",
    "i94port_df.drop(columns =['port'], inplace = True)\n",
    "\n",
    "# Convert pandas dataframe to list (objects which had single quotes removed automatically become string again with single quotes)\n",
    "i94port_data=i94port_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['ALC', 'ALCAN', 'AK']\n['ANC', 'ANCHORAGE', 'AK']\n['BAR', 'BAKER AAF - BAKER ISLAND', 'AK']\n"
     ]
    }
   ],
   "source": [
    "## print i94port_data list to check\n",
    "print(i94port_data[0])\n",
    "print(i94port_data[1])\n",
    "print(i94port_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      id                 port_city port_state\n0    ALC                     ALCAN         AK\n1    ANC                 ANCHORAGE         AK\n2    BAR  BAKER AAF - BAKER ISLAND         AK\n3    DAC             DALTONS CACHE         AK\n4    PIZ    DEW STATION PT LAY DEW         AK\n..   ...                       ...        ...\n655  ADU        No PORT Code (ADU)        NaN\n656  AKT        No PORT Code (AKT)        NaN\n657  LIT        No PORT Code (LIT)        NaN\n658  A2A        No PORT Code (A2A)        NaN\n659  OSN        No PORT Code (OSN)        NaN\n\n[660 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>port_city</th>\n      <th>port_state</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ALC</td>\n      <td>ALCAN</td>\n      <td>AK</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ANC</td>\n      <td>ANCHORAGE</td>\n      <td>AK</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BAR</td>\n      <td>BAKER AAF - BAKER ISLAND</td>\n      <td>AK</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DAC</td>\n      <td>DALTONS CACHE</td>\n      <td>AK</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PIZ</td>\n      <td>DEW STATION PT LAY DEW</td>\n      <td>AK</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>655</th>\n      <td>ADU</td>\n      <td>No PORT Code (ADU)</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>656</th>\n      <td>AKT</td>\n      <td>No PORT Code (AKT)</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>657</th>\n      <td>LIT</td>\n      <td>No PORT Code (LIT)</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>658</th>\n      <td>A2A</td>\n      <td>No PORT Code (A2A)</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>659</th>\n      <td>OSN</td>\n      <td>No PORT Code (OSN)</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>660 rows Ã— 3 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "## Take a look at the i94port_df created\n",
    "display(i94port_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now convert list to spark dataframe\n",
    "# Create a schema for the dataframe\n",
    "i94port_schema = StructType([\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('port_city', StringType(), True),\n",
    "    StructField('port_state', StringType(), True)\n",
    "])\n",
    "i94port=spark.createDataFrame(i94port_data, i94port_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "DataFrame[id: string, port_city: string, port_state: string]"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Display dataframe\n",
    "display(i94port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create parquet file\n",
    "i94port.write.mode('overwrite').parquet('./data/i94port.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read i94res text file\n",
    "i94res_df = pd.read_csv('./data/i94res_cit.txt',sep='=',names=['id','country'])\n",
    "# Remove whitespaces and single quotes\n",
    "i94res_df['country']=i94res_df['country'].str.replace(\"'\",'').str.strip()\n",
    "# Convert pandas dataframe to list (objects which had single quotes removed automatically become string again with single quotes)\n",
    "i94res_data=i94res_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[582, 'MEXICO Air Sea, and Not Reported (I-94, no land arrivals)']\n[236, 'AFGHANISTAN']\n[101, 'ALBANIA']\n"
     ]
    }
   ],
   "source": [
    "## print i94res_data list to check\n",
    "print(i94res_data[0])\n",
    "print(i94res_data[1])\n",
    "print(i94res_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now convert list to spark dataframe\n",
    "# Create a schema for the dataframe\n",
    "i94res_schema = StructType([\n",
    "    StructField('id', StringType(), True),\n",
    "    StructField('country', StringType(), True)\n",
    "])\n",
    "i94res=spark.createDataFrame(i94res_data, i94res_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "DataFrame[id: string, country: string]"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Display dataframe\n",
    "display(i94res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parquet file\n",
    "i94res.write.mode('overwrite').parquet('./data/i94res.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "/* I94VISA - Visa codes collapsed into three categories:\n",
    "   1 = Business\n",
    "   2 = Pleasure\n",
    "   3 = Student\n",
    "*/\n",
    "'''\n",
    "i94visa_data = [[1, 'Business'], [2, 'Pleasure'], [3, 'Student']]\n",
    "\n",
    "## Convert to spark dataframe\n",
    "i94visa=spark.createDataFrame(i94visa_data)\n",
    "\n",
    "# Create parquet file\n",
    "i94visa.write.mode('overwrite').parquet('./data/i94visa.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------+----------+----------+--------+-------+-------+------------+------------------+----------+---------+---------+----------+---------+\n|           City|State_Code|Median_age|Male_Pop|Fem_Pop|Ttl_Pop|Foreign-born|Avg_Household_Size|Native_Pop|Asian_Pop|Black_Pop|Latino_Pop|White_Pop|\n+---------------+----------+----------+--------+-------+-------+------------+------------------+----------+---------+---------+----------+---------+\n|Highlands Ranch|        CO|      39.6|   49186|  53281| 102467|        8827|              2.72|      1480|     5650|     1779|      8393|    94499|\n|           Kent|        WA|      33.4|   61825|  65137| 126962|       38175|              3.06|      3651|    26168|    20450|     21928|    67918|\n|        Madison|        WI|      30.7|  122596| 126360| 248956|       30090|              2.23|      2296|    23937|    20424|     19697|   204302|\n+---------------+----------+----------+--------+-------+-------+------------+------------------+----------+---------+---------+----------+---------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "us.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['i94res',\n",
       " 'i94port',\n",
       " 'arrdate',\n",
       " 'i94mode',\n",
       " 'depdate',\n",
       " 'i94bir',\n",
       " 'i94visa',\n",
       " 'count',\n",
       " 'gender',\n",
       " 'admnum']"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "i94_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3096302"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "# Add i94port city and state columns to i94 dataframe\n",
    "i94_spark.join(i94port, i94_spark.i94port==i94port.id, how='left').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count above matches original i94 cleaned dataset `i94_spark`\n",
    "#   Commit i94_spark\n",
    "i94_spark=i94_spark.join(i94port, i94_spark.i94port==i94port.id, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['i94res',\n",
       " 'i94port',\n",
       " 'arrdate',\n",
       " 'i94mode',\n",
       " 'depdate',\n",
       " 'i94bir',\n",
       " 'i94visa',\n",
       " 'count',\n",
       " 'gender',\n",
       " 'admnum',\n",
       " 'id',\n",
       " 'port_city',\n",
       " 'port_state']"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "i94_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---+---------+----------+\n|i94res|i94port|arrdate|i94mode|depdate|i94bir|i94visa|count|gender|     admnum| id|port_city|port_state|\n+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---+---------+----------+\n|   110|    BGM|  20550|      1|  20556|    36|      1|    1|     F|92847893530|BGM|   BANGOR|        ME|\n|   108|    BGM|  20569|      1|  20571|    43|      1|    1|     M|59234302533|BGM|   BANGOR|        ME|\n|   129|    BGM|  20559|      1|  20584|    67|      1|    1|     M|93553405030|BGM|   BANGOR|        ME|\n|   261|    BGM|  20568|      1|   null|     9|      1|    1|     M|94455571030|BGM|   BANGOR|        ME|\n|   111|    BGM|  20563|      1|  20567|    56|      1|    1|     M|93951299930|BGM|   BANGOR|        ME|\n|   135|    BGM|  20564|      1|  20569|    30|      2|    1|     M|94033156330|BGM|   BANGOR|        ME|\n|   582|    BGM|  20563|      1|  20575|    57|      1|    1|     M|93924664230|BGM|   BANGOR|        ME|\n|   373|    BGM|  20547|      1|  20549|    55|      1|    1|     M|92692756930|BGM|   BANGOR|        ME|\n|   127|    BGM|  20571|      1|  20661|    27|      1|    1|     F|94670010530|BGM|   BANGOR|        ME|\n|   582|    BGM|  20566|      1|  20567|    42|      1|    1|     M|94251980230|BGM|   BANGOR|        ME|\n|   124|    BGM|  20547|      1|  20549|    38|      1|    1|     M|92692552430|BGM|   BANGOR|        ME|\n|   135|    BGM|  20564|      1|  20569|    33|      2|    1|     M|94033775030|BGM|   BANGOR|        ME|\n|   108|    BGM|  20569|      1|  20571|    40|      1|    1|     M|94510578330|BGM|   BANGOR|        ME|\n|   135|    BGM|  20565|      1|  20581|    34|      2|    1|     F|56546213233|BGM|   BANGOR|        ME|\n|   135|    BGM|  20547|      1|  20687|    69|      1|    1|     M|92664011030|BGM|   BANGOR|        ME|\n|   158|    BGM|  20547|      1|  20550|    15|      2|    1|     F|92609680230|BGM|   BANGOR|        ME|\n|   135|    BGM|  20556|      1|  20557|    63|      1|    1|     M|93334238330|BGM|   BANGOR|        ME|\n|   112|    BGM|  20566|      1|  20581|    45|      1|    1|     M|94264768730|BGM|   BANGOR|        ME|\n|   135|    BGM|  20547|      1|  20678|    62|      2|    1|     M|92648354430|BGM|   BANGOR|        ME|\n|   135|    BGM|  20564|      1|  20569|    36|      2|    1|     M|94033624830|BGM|   BANGOR|        ME|\n+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---+---------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "i94_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop `id` column\n",
    "i94_spark=i94_spark.drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['i94res',\n",
       " 'i94port',\n",
       " 'arrdate',\n",
       " 'i94mode',\n",
       " 'depdate',\n",
       " 'i94bir',\n",
       " 'i94visa',\n",
       " 'count',\n",
       " 'gender',\n",
       " 'admnum',\n",
       " 'port_city',\n",
       " 'port_state']"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "i94_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['City',\n",
       " 'State_Code',\n",
       " 'Median_age',\n",
       " 'Male_Pop',\n",
       " 'Fem_Pop',\n",
       " 'Ttl_Pop',\n",
       " 'Foreign-born',\n",
       " 'Avg_Household_Size',\n",
       " 'Native_Pop',\n",
       " 'Asian_Pop',\n",
       " 'Black_Pop',\n",
       " 'Latino_Pop',\n",
       " 'White_Pop']"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "us.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will join `us` with `i94_spark` to get fact table 'i94non_immigrant_port_entry`\n",
    "# NOTE: We use left join againt city records which may cause null values because\n",
    "# we may not currently have demographic stats on all U.S. ports of entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94non_immigrant_port_entry=i94_spark.join(us, (upper(i94_spark.port_city)==upper(us.City)) & \\\n",
    "                                           (upper(i94_spark.port_state)==upper(us.State_Code)), how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['i94res',\n",
       " 'i94port',\n",
       " 'arrdate',\n",
       " 'i94mode',\n",
       " 'depdate',\n",
       " 'i94bir',\n",
       " 'i94visa',\n",
       " 'count',\n",
       " 'gender',\n",
       " 'admnum',\n",
       " 'port_city',\n",
       " 'port_state',\n",
       " 'City',\n",
       " 'State_Code',\n",
       " 'Median_age',\n",
       " 'Male_Pop',\n",
       " 'Fem_Pop',\n",
       " 'Ttl_Pop',\n",
       " 'Foreign-born',\n",
       " 'Avg_Household_Size',\n",
       " 'Native_Pop',\n",
       " 'Asian_Pop',\n",
       " 'Black_Pop',\n",
       " 'Latino_Pop',\n",
       " 'White_Pop']"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "i94non_immigrant_port_entry.count()\n",
    "\n",
    "i94non_immigrant_port_entry.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop City and State_Code\n",
    "i94non_immigrant_port_entry=i94non_immigrant_port_entry.drop(\"City\",\"State_Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---------+----------+----------+--------+-------+-------+------------+------------------+----------+---------+---------+----------+---------+\n|i94res|i94port|arrdate|i94mode|depdate|i94bir|i94visa|count|gender|     admnum|port_city|port_state|Median_age|Male_Pop|Fem_Pop|Ttl_Pop|Foreign-born|Avg_Household_Size|Native_Pop|Asian_Pop|Black_Pop|Latino_Pop|White_Pop|\n+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---------+----------+----------+--------+-------+-------+------------+------------------+----------+---------+---------+----------+---------+\n|   103|    NEC|  20556|      3|  20557|    51|      2|    1|     F|  788711085|    NECHE|        ND|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   112|    NEC|  20573|      3|  20575|    32|      2|    1|     F|59477349333|    NECHE|        ND|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   245|    LEW|  20574|      3|  20576|    21|      2|    1|     F|94960311530| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   245|    LEW|  20546|      3|   null|    27|      3|    1|     F|71136033030| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   263|    LEW|  20559|      3|  20561|    38|      2|    1|  null|93151263430| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   514|    LEW|  20574|      3|  20596|    38|      2|    1|     F|94972653530| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   112|    LEW|  20560|      3|   null|    48|      2|    1|     M|17269695927| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   514|    LEW|  20552|      3|  20554|    52|      2|    1|     F|  745535185| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   260|    LEW|  20559|      3|  20561|    53|      2|    1|     F|80354257630| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   206|    LEW|  20549|      3|  20554|    20|      2|    1|     F|91281323030| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   213|    LEW|  20573|      3|  20575|    31|      2|    1|     F|79211218230| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   274|    LEW|  20573|      3|  20576|    31|      2|    1|     M|94859874130| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   213|    LEW|  20573|      3|  20576|    34|      2|    1|     M|94831519930| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   276|    LEW|  20560|      3|  20574|    35|      3|    1|     F|77790902930| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   111|    LEW|  20570|      3|  20576|    47|      2|    1|     M|59315035133| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   213|    LEW|  20571|      3|  20622|    47|      2|    1|     F|  751570585| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   135|    LEW|  20558|      3|  20560|    52|      2|    1|     M|55840487133| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   135|    LEW|  20560|      3|  20561|    53|      2|    1|     M|54209393433| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   514|    LEW|  20574|      3|  20575|    56|      2|    1|     F|80374948630| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n|   111|    LEW|  20562|      3|  20565|    57|      1|    1|     M|56399954533| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|\n+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---------+----------+----------+--------+-------+-------+------------+------------------+----------+---------+---------+----------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Show i94non_immigrant_port_entry\n",
    "i94non_immigrant_port_entry.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the i94date dimension from the i94non_immigrant_port_entry dataframe:\n",
    "# Convert SAS arrival date to datetime format - add it as last column\n",
    "get_date = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "i94non_immigrant_port_entry = i94non_immigrant_port_entry.withColumn(\"arrival_date\", get_date(i94non_immigrant_port_entry.arrdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---------+----------+----------+--------+-------+-------+------------+------------------+----------+---------+---------+----------+---------+------------+\n|i94res|i94port|arrdate|i94mode|depdate|i94bir|i94visa|count|gender|     admnum|port_city|port_state|Median_age|Male_Pop|Fem_Pop|Ttl_Pop|Foreign-born|Avg_Household_Size|Native_Pop|Asian_Pop|Black_Pop|Latino_Pop|White_Pop|arrival_date|\n+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---------+----------+----------+--------+-------+-------+------------+------------------+----------+---------+---------+----------+---------+------------+\n|   103|    NEC|  20556|      3|  20557|    51|      2|    1|     F|  788711085|    NECHE|        ND|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|  2016-04-12|\n|   112|    NEC|  20573|      3|  20575|    32|      2|    1|     F|59477349333|    NECHE|        ND|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|  2016-04-29|\n|   245|    LEW|  20574|      3|  20576|    21|      2|    1|     F|94960311530| LEWISTON|        NY|      null|    null|   null|   null|        null|              null|      null|     null|     null|      null|     null|  2016-04-30|\n+------+-------+-------+-------+-------+------+-------+-----+------+-----------+---------+----------+----------+--------+-------+-------+------------+------------------+----------+---------+---------+----------+---------+------------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "i94non_immigrant_port_entry.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "i94date=i94non_immigrant_port_entry.select(col('arrdate').alias('arrival_sasdate'),\n",
    "                                           col('arrival_date').alias('arrival_iso_date'),\n",
    "                                           date_format('arrival_date','M').alias('arrival_month'),\n",
    "                                           date_format('arrival_date','E').alias('arrival_dayofweek'), \n",
    "                                           date_format('arrival_date', 'y').alias('arrival_year'), \n",
    "                                           date_format('arrival_date', 'd').alias('arrival_day'),\n",
    "                                           date_format('arrival_date','w').alias('arrival_weekofyear')).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop arrival_date column from the i94non_immigrant_port_entry dataframe and finally save it to parquet file \n",
    "i94non_immigrant_port_entry.drop('arrival_date').write.mode(\"overwrite\").parquet('./data/i94non_immigrant_port_entry.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o362.count.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition, ENSURE_REQUIREMENTS, [id=#2390]\n+- *(15) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#1969L])\n   +- *(15) HashAggregate(keys=[arrival_day#1869, arrival_iso_date#1865, arrival_month#1866, arrival_dayofweek#1867, arrival_weekofyear#1870, arrival_year#1868, arrival_sasdate#1864], functions=[], output=[])\n      +- Exchange hashpartitioning(arrival_day#1869, arrival_iso_date#1865, arrival_month#1866, arrival_dayofweek#1867, arrival_weekofyear#1870, arrival_year#1868, arrival_sasdate#1864, 200), ENSURE_REQUIREMENTS, [id=#2385]\n         +- *(14) HashAggregate(keys=[arrival_day#1869, arrival_iso_date#1865, arrival_month#1866, arrival_dayofweek#1867, arrival_weekofyear#1870, arrival_year#1868, arrival_sasdate#1864], functions=[], output=[arrival_day#1869, arrival_iso_date#1865, arrival_month#1866, arrival_dayofweek#1867, arrival_weekofyear#1870, arrival_year#1868, arrival_sasdate#1864])\n            +- *(14) Project [arrdate#1057 AS arrival_sasdate#1864, pythonUDF0#1967 AS arrival_iso_date#1865, date_format(cast(pythonUDF0#1967 as timestamp), M, Some(Asia/Singapore)) AS arrival_month#1866, date_format(cast(pythonUDF0#1967 as timestamp), E, Some(Asia/Singapore)) AS arrival_dayofweek#1867, date_format(cast(pythonUDF0#1967 as timestamp), y, Some(Asia/Singapore)) AS arrival_year#1868, date_format(cast(pythonUDF0#1967 as timestamp), d, Some(Asia/Singapore)) AS arrival_day#1869, date_format(cast(pythonUDF0#1967 as timestamp), w, Some(Asia/Singapore)) AS arrival_weekofyear#1870]\n               +- BatchEvalPython [<lambda>(arrdate#1057)], [pythonUDF0#1967]\n                  +- *(13) Project [arrdate#1057]\n                     +- SortMergeJoin [upper(port_city#1226), upper(port_state#1227)], [upper(City#16), upper(State_Code#688)], LeftOuter\n                        :- *(7) Sort [upper(port_city#1226) ASC NULLS FIRST, upper(port_state#1227) ASC NULLS FIRST], false, 0\n                        :  +- Exchange hashpartitioning(upper(port_city#1226), upper(port_state#1227), 200), ENSURE_REQUIREMENTS, [id=#2348]\n                        :     +- *(6) Project [arrdate#1057, port_city#1226, port_state#1227]\n                        :        +- SortMergeJoin [i94port#953], [id#1225], LeftOuter\n                        :           :- *(3) Sort [i94port#953 ASC NULLS FIRST], false, 0\n                        :           :  +- Exchange hashpartitioning(i94port#953, 200), ENSURE_REQUIREMENTS, [id=#2335]\n                        :           :     +- *(2) HashAggregate(keys=[count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970], functions=[], output=[i94port#953, arrdate#1057])\n                        :           :        +- Exchange hashpartitioning(count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970, 200), ENSURE_REQUIREMENTS, [id=#2331]\n                        :           :           +- *(1) HashAggregate(keys=[count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970], functions=[], output=[count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970])\n                        :           :              +- *(1) Project [cast(i94res#952 as int) AS i94res#1056, i94port#953, cast(arrdate#954 as int) AS arrdate#1057, cast(i94mode#955 as int) AS i94mode#1058, cast(depdate#957 as int) AS depdate#1059, cast(i94bir#958 as int) AS i94bir#1060, cast(i94visa#959 as int) AS i94visa#1061, cast(count#960 as int) AS count#1062, gender#970, cast(admnum#973 as bigint) AS admnum#1063L]\n                        :           :                 +- *(1) ColumnarToRow\n                        :           :                    +- FileScan parquet [i94res#952,i94port#953,arrdate#954,i94mode#955,depdate#957,i94bir#958,i94visa#959,count#960,gender#970,admnum#973] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/sas_data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i94res:double,i94port:string,arrdate:double,i94mode:double,depdate:double,i94bir:double,i9...\n                        :           +- *(5) Sort [id#1225 ASC NULLS FIRST], false, 0\n                        :              +- Exchange hashpartitioning(id#1225, 200), ENSURE_REQUIREMENTS, [id=#2340]\n                        :                 +- *(4) Filter isnotnull(id#1225)\n                        :                    +- *(4) Scan ExistingRDD[id#1225,port_city#1226,port_state#1227]\n                        +- *(12) Sort [upper(City#16) ASC NULLS FIRST, upper(State_Code#688) ASC NULLS FIRST], false, 0\n                           +- Exchange hashpartitioning(upper(City#16), upper(State_Code#688), 200), ENSURE_REQUIREMENTS, [id=#2372]\n                              +- *(11) Project [City#16, State Code#25 AS State_Code#688]\n                                 +- *(11) BroadcastHashJoin [City#16, State Code#25], [City#542, State Code#551], Inner, BuildRight, false\n                                    :- *(11) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[City#16, State Code#25])\n                                    :  +- Exchange hashpartitioning(Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24, 200), ENSURE_REQUIREMENTS, [id=#2356]\n                                    :     +- *(8) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24])\n                                    :        +- *(8) Filter (isnotnull(City#16) AND isnotnull(State Code#25))\n                                    :           +- FileScan csv [City#16,State#17,Median Age#18,Male Population#19,Female Population#20,Total Population#21,Foreign-born#23,Average Household Size#24,State Code#25] Batched: false, DataFilters: [isnotnull(City#16), isnotnull(State Code#25)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State:string,Median Age:string,Male Population:string,Female Population:string...\n                                    +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true], input[1, string, true]),false), [id=#2367]\n                                       +- *(10) HashAggregate(keys=[City#542, State Code#551], functions=[], output=[City#542, State Code#551])\n                                          +- Exchange hashpartitioning(City#542, State Code#551, 200), ENSURE_REQUIREMENTS, [id=#2363]\n                                             +- *(9) HashAggregate(keys=[City#542, State Code#551], functions=[], output=[City#542, State Code#551])\n                                                +- *(9) Filter (isnotnull(City#542) AND isnotnull(State Code#551))\n                                                   +- FileScan csv [City#542,State Code#551] Batched: false, DataFilters: [isnotnull(City#542), isnotnull(State Code#551)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State Code:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:387)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3006)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3005)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3005)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(arrival_day#1869, arrival_iso_date#1865, arrival_month#1866, arrival_dayofweek#1867, arrival_weekofyear#1870, arrival_year#1868, arrival_sasdate#1864, 200), ENSURE_REQUIREMENTS, [id=#2385]\n+- *(14) HashAggregate(keys=[arrival_day#1869, arrival_iso_date#1865, arrival_month#1866, arrival_dayofweek#1867, arrival_weekofyear#1870, arrival_year#1868, arrival_sasdate#1864], functions=[], output=[arrival_day#1869, arrival_iso_date#1865, arrival_month#1866, arrival_dayofweek#1867, arrival_weekofyear#1870, arrival_year#1868, arrival_sasdate#1864])\n   +- *(14) Project [arrdate#1057 AS arrival_sasdate#1864, pythonUDF0#1967 AS arrival_iso_date#1865, date_format(cast(pythonUDF0#1967 as timestamp), M, Some(Asia/Singapore)) AS arrival_month#1866, date_format(cast(pythonUDF0#1967 as timestamp), E, Some(Asia/Singapore)) AS arrival_dayofweek#1867, date_format(cast(pythonUDF0#1967 as timestamp), y, Some(Asia/Singapore)) AS arrival_year#1868, date_format(cast(pythonUDF0#1967 as timestamp), d, Some(Asia/Singapore)) AS arrival_day#1869, date_format(cast(pythonUDF0#1967 as timestamp), w, Some(Asia/Singapore)) AS arrival_weekofyear#1870]\n      +- BatchEvalPython [<lambda>(arrdate#1057)], [pythonUDF0#1967]\n         +- *(13) Project [arrdate#1057]\n            +- SortMergeJoin [upper(port_city#1226), upper(port_state#1227)], [upper(City#16), upper(State_Code#688)], LeftOuter\n               :- *(7) Sort [upper(port_city#1226) ASC NULLS FIRST, upper(port_state#1227) ASC NULLS FIRST], false, 0\n               :  +- Exchange hashpartitioning(upper(port_city#1226), upper(port_state#1227), 200), ENSURE_REQUIREMENTS, [id=#2348]\n               :     +- *(6) Project [arrdate#1057, port_city#1226, port_state#1227]\n               :        +- SortMergeJoin [i94port#953], [id#1225], LeftOuter\n               :           :- *(3) Sort [i94port#953 ASC NULLS FIRST], false, 0\n               :           :  +- Exchange hashpartitioning(i94port#953, 200), ENSURE_REQUIREMENTS, [id=#2335]\n               :           :     +- *(2) HashAggregate(keys=[count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970], functions=[], output=[i94port#953, arrdate#1057])\n               :           :        +- Exchange hashpartitioning(count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970, 200), ENSURE_REQUIREMENTS, [id=#2331]\n               :           :           +- *(1) HashAggregate(keys=[count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970], functions=[], output=[count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970])\n               :           :              +- *(1) Project [cast(i94res#952 as int) AS i94res#1056, i94port#953, cast(arrdate#954 as int) AS arrdate#1057, cast(i94mode#955 as int) AS i94mode#1058, cast(depdate#957 as int) AS depdate#1059, cast(i94bir#958 as int) AS i94bir#1060, cast(i94visa#959 as int) AS i94visa#1061, cast(count#960 as int) AS count#1062, gender#970, cast(admnum#973 as bigint) AS admnum#1063L]\n               :           :                 +- *(1) ColumnarToRow\n               :           :                    +- FileScan parquet [i94res#952,i94port#953,arrdate#954,i94mode#955,depdate#957,i94bir#958,i94visa#959,count#960,gender#970,admnum#973] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/sas_data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i94res:double,i94port:string,arrdate:double,i94mode:double,depdate:double,i94bir:double,i9...\n               :           +- *(5) Sort [id#1225 ASC NULLS FIRST], false, 0\n               :              +- Exchange hashpartitioning(id#1225, 200), ENSURE_REQUIREMENTS, [id=#2340]\n               :                 +- *(4) Filter isnotnull(id#1225)\n               :                    +- *(4) Scan ExistingRDD[id#1225,port_city#1226,port_state#1227]\n               +- *(12) Sort [upper(City#16) ASC NULLS FIRST, upper(State_Code#688) ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(upper(City#16), upper(State_Code#688), 200), ENSURE_REQUIREMENTS, [id=#2372]\n                     +- *(11) Project [City#16, State Code#25 AS State_Code#688]\n                        +- *(11) BroadcastHashJoin [City#16, State Code#25], [City#542, State Code#551], Inner, BuildRight, false\n                           :- *(11) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[City#16, State Code#25])\n                           :  +- Exchange hashpartitioning(Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24, 200), ENSURE_REQUIREMENTS, [id=#2356]\n                           :     +- *(8) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24])\n                           :        +- *(8) Filter (isnotnull(City#16) AND isnotnull(State Code#25))\n                           :           +- FileScan csv [City#16,State#17,Median Age#18,Male Population#19,Female Population#20,Total Population#21,Foreign-born#23,Average Household Size#24,State Code#25] Batched: false, DataFilters: [isnotnull(City#16), isnotnull(State Code#25)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State:string,Median Age:string,Male Population:string,Female Population:string...\n                           +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true], input[1, string, true]),false), [id=#2367]\n                              +- *(10) HashAggregate(keys=[City#542, State Code#551], functions=[], output=[City#542, State Code#551])\n                                 +- Exchange hashpartitioning(City#542, State Code#551, 200), ENSURE_REQUIREMENTS, [id=#2363]\n                                    +- *(9) HashAggregate(keys=[City#542, State Code#551], functions=[], output=[City#542, State Code#551])\n                                       +- *(9) Filter (isnotnull(City#542) AND isnotnull(State Code#551))\n                                          +- FileScan csv [City#542,State Code#551] Batched: false, DataFilters: [isnotnull(City#542), isnotnull(State Code#551)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State Code:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 40 more\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'w' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:196)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:185)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:109)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:300)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:333)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter(datetimeExpressions.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter$(datetimeExpressions.scala:67)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.getFormatter(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.$anonfun$formatterOption$1(datetimeExpressions.scala:64)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption(datetimeExpressions.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption$(datetimeExpressions.scala:62)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption$lzycompute(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.doGenCode(datetimeExpressions.scala:790)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:163)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$2(basicPhysicalOperators.scala:73)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$1(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1026)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:733)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:47)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 64 more\nCaused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: w, Please use the SQL function EXTRACT instead\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4(DateTimeFormatterHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4$adapted(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.immutable.StringOps.foreach(StringOps.scala:33)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$2(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.convertIncompatiblePattern(DateTimeFormatterHelper.scala:318)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:121)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:59)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:68)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:67)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:108)\n\t... 134 more\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-4fb16bfb7de1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mi94date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mi94date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \"\"\"\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o362.count.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition, ENSURE_REQUIREMENTS, [id=#2390]\n+- *(15) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#1969L])\n   +- *(15) HashAggregate(keys=[arrival_day#1869, arrival_iso_date#1865, arrival_month#1866, arrival_dayofweek#1867, arrival_weekofyear#1870, arrival_year#1868, arrival_sasdate#1864], functions=[], output=[])\n      +- Exchange hashpartitioning(arrival_day#1869, arrival_iso_date#1865, arrival_month#1866, arrival_dayofweek#1867, arrival_weekofyear#1870, arrival_year#1868, arrival_sasdate#1864, 200), ENSURE_REQUIREMENTS, [id=#2385]\n         +- *(14) HashAggregate(keys=[arrival_day#1869, arrival_iso_date#1865, arrival_month#1866, arrival_dayofweek#1867, arrival_weekofyear#1870, arrival_year#1868, arrival_sasdate#1864], functions=[], output=[arrival_day#1869, arrival_iso_date#1865, arrival_month#1866, arrival_dayofweek#1867, arrival_weekofyear#1870, arrival_year#1868, arrival_sasdate#1864])\n            +- *(14) Project [arrdate#1057 AS arrival_sasdate#1864, pythonUDF0#1967 AS arrival_iso_date#1865, date_format(cast(pythonUDF0#1967 as timestamp), M, Some(Asia/Singapore)) AS arrival_month#1866, date_format(cast(pythonUDF0#1967 as timestamp), E, Some(Asia/Singapore)) AS arrival_dayofweek#1867, date_format(cast(pythonUDF0#1967 as timestamp), y, Some(Asia/Singapore)) AS arrival_year#1868, date_format(cast(pythonUDF0#1967 as timestamp), d, Some(Asia/Singapore)) AS arrival_day#1869, date_format(cast(pythonUDF0#1967 as timestamp), w, Some(Asia/Singapore)) AS arrival_weekofyear#1870]\n               +- BatchEvalPython [<lambda>(arrdate#1057)], [pythonUDF0#1967]\n                  +- *(13) Project [arrdate#1057]\n                     +- SortMergeJoin [upper(port_city#1226), upper(port_state#1227)], [upper(City#16), upper(State_Code#688)], LeftOuter\n                        :- *(7) Sort [upper(port_city#1226) ASC NULLS FIRST, upper(port_state#1227) ASC NULLS FIRST], false, 0\n                        :  +- Exchange hashpartitioning(upper(port_city#1226), upper(port_state#1227), 200), ENSURE_REQUIREMENTS, [id=#2348]\n                        :     +- *(6) Project [arrdate#1057, port_city#1226, port_state#1227]\n                        :        +- SortMergeJoin [i94port#953], [id#1225], LeftOuter\n                        :           :- *(3) Sort [i94port#953 ASC NULLS FIRST], false, 0\n                        :           :  +- Exchange hashpartitioning(i94port#953, 200), ENSURE_REQUIREMENTS, [id=#2335]\n                        :           :     +- *(2) HashAggregate(keys=[count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970], functions=[], output=[i94port#953, arrdate#1057])\n                        :           :        +- Exchange hashpartitioning(count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970, 200), ENSURE_REQUIREMENTS, [id=#2331]\n                        :           :           +- *(1) HashAggregate(keys=[count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970], functions=[], output=[count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970])\n                        :           :              +- *(1) Project [cast(i94res#952 as int) AS i94res#1056, i94port#953, cast(arrdate#954 as int) AS arrdate#1057, cast(i94mode#955 as int) AS i94mode#1058, cast(depdate#957 as int) AS depdate#1059, cast(i94bir#958 as int) AS i94bir#1060, cast(i94visa#959 as int) AS i94visa#1061, cast(count#960 as int) AS count#1062, gender#970, cast(admnum#973 as bigint) AS admnum#1063L]\n                        :           :                 +- *(1) ColumnarToRow\n                        :           :                    +- FileScan parquet [i94res#952,i94port#953,arrdate#954,i94mode#955,depdate#957,i94bir#958,i94visa#959,count#960,gender#970,admnum#973] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/sas_data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i94res:double,i94port:string,arrdate:double,i94mode:double,depdate:double,i94bir:double,i9...\n                        :           +- *(5) Sort [id#1225 ASC NULLS FIRST], false, 0\n                        :              +- Exchange hashpartitioning(id#1225, 200), ENSURE_REQUIREMENTS, [id=#2340]\n                        :                 +- *(4) Filter isnotnull(id#1225)\n                        :                    +- *(4) Scan ExistingRDD[id#1225,port_city#1226,port_state#1227]\n                        +- *(12) Sort [upper(City#16) ASC NULLS FIRST, upper(State_Code#688) ASC NULLS FIRST], false, 0\n                           +- Exchange hashpartitioning(upper(City#16), upper(State_Code#688), 200), ENSURE_REQUIREMENTS, [id=#2372]\n                              +- *(11) Project [City#16, State Code#25 AS State_Code#688]\n                                 +- *(11) BroadcastHashJoin [City#16, State Code#25], [City#542, State Code#551], Inner, BuildRight, false\n                                    :- *(11) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[City#16, State Code#25])\n                                    :  +- Exchange hashpartitioning(Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24, 200), ENSURE_REQUIREMENTS, [id=#2356]\n                                    :     +- *(8) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24])\n                                    :        +- *(8) Filter (isnotnull(City#16) AND isnotnull(State Code#25))\n                                    :           +- FileScan csv [City#16,State#17,Median Age#18,Male Population#19,Female Population#20,Total Population#21,Foreign-born#23,Average Household Size#24,State Code#25] Batched: false, DataFilters: [isnotnull(City#16), isnotnull(State Code#25)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State:string,Median Age:string,Male Population:string,Female Population:string...\n                                    +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true], input[1, string, true]),false), [id=#2367]\n                                       +- *(10) HashAggregate(keys=[City#542, State Code#551], functions=[], output=[City#542, State Code#551])\n                                          +- Exchange hashpartitioning(City#542, State Code#551, 200), ENSURE_REQUIREMENTS, [id=#2363]\n                                             +- *(9) HashAggregate(keys=[City#542, State Code#551], functions=[], output=[City#542, State Code#551])\n                                                +- *(9) Filter (isnotnull(City#542) AND isnotnull(State Code#551))\n                                                   +- FileScan csv [City#542,State Code#551] Batched: false, DataFilters: [isnotnull(City#542), isnotnull(State Code#551)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State Code:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:387)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3006)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3005)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3005)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(arrival_day#1869, arrival_iso_date#1865, arrival_month#1866, arrival_dayofweek#1867, arrival_weekofyear#1870, arrival_year#1868, arrival_sasdate#1864, 200), ENSURE_REQUIREMENTS, [id=#2385]\n+- *(14) HashAggregate(keys=[arrival_day#1869, arrival_iso_date#1865, arrival_month#1866, arrival_dayofweek#1867, arrival_weekofyear#1870, arrival_year#1868, arrival_sasdate#1864], functions=[], output=[arrival_day#1869, arrival_iso_date#1865, arrival_month#1866, arrival_dayofweek#1867, arrival_weekofyear#1870, arrival_year#1868, arrival_sasdate#1864])\n   +- *(14) Project [arrdate#1057 AS arrival_sasdate#1864, pythonUDF0#1967 AS arrival_iso_date#1865, date_format(cast(pythonUDF0#1967 as timestamp), M, Some(Asia/Singapore)) AS arrival_month#1866, date_format(cast(pythonUDF0#1967 as timestamp), E, Some(Asia/Singapore)) AS arrival_dayofweek#1867, date_format(cast(pythonUDF0#1967 as timestamp), y, Some(Asia/Singapore)) AS arrival_year#1868, date_format(cast(pythonUDF0#1967 as timestamp), d, Some(Asia/Singapore)) AS arrival_day#1869, date_format(cast(pythonUDF0#1967 as timestamp), w, Some(Asia/Singapore)) AS arrival_weekofyear#1870]\n      +- BatchEvalPython [<lambda>(arrdate#1057)], [pythonUDF0#1967]\n         +- *(13) Project [arrdate#1057]\n            +- SortMergeJoin [upper(port_city#1226), upper(port_state#1227)], [upper(City#16), upper(State_Code#688)], LeftOuter\n               :- *(7) Sort [upper(port_city#1226) ASC NULLS FIRST, upper(port_state#1227) ASC NULLS FIRST], false, 0\n               :  +- Exchange hashpartitioning(upper(port_city#1226), upper(port_state#1227), 200), ENSURE_REQUIREMENTS, [id=#2348]\n               :     +- *(6) Project [arrdate#1057, port_city#1226, port_state#1227]\n               :        +- SortMergeJoin [i94port#953], [id#1225], LeftOuter\n               :           :- *(3) Sort [i94port#953 ASC NULLS FIRST], false, 0\n               :           :  +- Exchange hashpartitioning(i94port#953, 200), ENSURE_REQUIREMENTS, [id=#2335]\n               :           :     +- *(2) HashAggregate(keys=[count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970], functions=[], output=[i94port#953, arrdate#1057])\n               :           :        +- Exchange hashpartitioning(count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970, 200), ENSURE_REQUIREMENTS, [id=#2331]\n               :           :           +- *(1) HashAggregate(keys=[count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970], functions=[], output=[count#1062, i94mode#1058, i94bir#1060, i94port#953, arrdate#1057, admnum#1063L, depdate#1059, i94visa#1061, i94res#1056, gender#970])\n               :           :              +- *(1) Project [cast(i94res#952 as int) AS i94res#1056, i94port#953, cast(arrdate#954 as int) AS arrdate#1057, cast(i94mode#955 as int) AS i94mode#1058, cast(depdate#957 as int) AS depdate#1059, cast(i94bir#958 as int) AS i94bir#1060, cast(i94visa#959 as int) AS i94visa#1061, cast(count#960 as int) AS count#1062, gender#970, cast(admnum#973 as bigint) AS admnum#1063L]\n               :           :                 +- *(1) ColumnarToRow\n               :           :                    +- FileScan parquet [i94res#952,i94port#953,arrdate#954,i94mode#955,depdate#957,i94bir#958,i94visa#959,count#960,gender#970,admnum#973] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/sas_data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i94res:double,i94port:string,arrdate:double,i94mode:double,depdate:double,i94bir:double,i9...\n               :           +- *(5) Sort [id#1225 ASC NULLS FIRST], false, 0\n               :              +- Exchange hashpartitioning(id#1225, 200), ENSURE_REQUIREMENTS, [id=#2340]\n               :                 +- *(4) Filter isnotnull(id#1225)\n               :                    +- *(4) Scan ExistingRDD[id#1225,port_city#1226,port_state#1227]\n               +- *(12) Sort [upper(City#16) ASC NULLS FIRST, upper(State_Code#688) ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(upper(City#16), upper(State_Code#688), 200), ENSURE_REQUIREMENTS, [id=#2372]\n                     +- *(11) Project [City#16, State Code#25 AS State_Code#688]\n                        +- *(11) BroadcastHashJoin [City#16, State Code#25], [City#542, State Code#551], Inner, BuildRight, false\n                           :- *(11) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[City#16, State Code#25])\n                           :  +- Exchange hashpartitioning(Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24, 200), ENSURE_REQUIREMENTS, [id=#2356]\n                           :     +- *(8) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24])\n                           :        +- *(8) Filter (isnotnull(City#16) AND isnotnull(State Code#25))\n                           :           +- FileScan csv [City#16,State#17,Median Age#18,Male Population#19,Female Population#20,Total Population#21,Foreign-born#23,Average Household Size#24,State Code#25] Batched: false, DataFilters: [isnotnull(City#16), isnotnull(State Code#25)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State:string,Median Age:string,Male Population:string,Female Population:string...\n                           +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true], input[1, string, true]),false), [id=#2367]\n                              +- *(10) HashAggregate(keys=[City#542, State Code#551], functions=[], output=[City#542, State Code#551])\n                                 +- Exchange hashpartitioning(City#542, State Code#551, 200), ENSURE_REQUIREMENTS, [id=#2363]\n                                    +- *(9) HashAggregate(keys=[City#542, State Code#551], functions=[], output=[City#542, State Code#551])\n                                       +- *(9) Filter (isnotnull(City#542) AND isnotnull(State Code#551))\n                                          +- FileScan csv [City#542,State Code#551] Batched: false, DataFilters: [isnotnull(City#542), isnotnull(State Code#551)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State Code:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 40 more\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'w' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:196)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:185)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:109)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:300)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:333)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter(datetimeExpressions.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter$(datetimeExpressions.scala:67)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.getFormatter(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.$anonfun$formatterOption$1(datetimeExpressions.scala:64)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption(datetimeExpressions.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption$(datetimeExpressions.scala:62)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption$lzycompute(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.doGenCode(datetimeExpressions.scala:790)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:163)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$2(basicPhysicalOperators.scala:73)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$1(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1026)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:733)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:47)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 64 more\nCaused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: w, Please use the SQL function EXTRACT instead\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4(DateTimeFormatterHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4$adapted(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.immutable.StringOps.foreach(StringOps.scala:33)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$2(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.convertIncompatiblePattern(DateTimeFormatterHelper.scala:318)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:121)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:59)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:68)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:67)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:108)\n\t... 134 more\n"
     ]
    }
   ],
   "source": [
    "i94date.count()\n",
    "i94date.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary sql table\n",
    "i94date.createOrReplaceTempView(\"i94date_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add seasons to i94 date dimension table\n",
    "i94date_season=spark.sql('''select arrival_sasdate,\n",
    "                         arrival_iso_date,\n",
    "                         arrival_month,\n",
    "                         arrival_dayofweek,\n",
    "                         arrival_year,\n",
    "                         arrival_day,\n",
    "                         arrival_weekofyear,\n",
    "                         CASE WHEN arrival_month IN (12, 1, 2) THEN 'winter' \n",
    "                                WHEN arrival_month IN (3, 4, 5) THEN 'spring' \n",
    "                                WHEN arrival_month IN (6, 7, 8) THEN 'summer' \n",
    "                                ELSE 'autumn' \n",
    "                         END AS date_season from i94date_table''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1241.showString.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(arrival_day#3551, arrival_iso_date#3547, arrival_month#3548, arrival_dayofweek#3549, arrival_weekofyear#3552, arrival_year#3550, arrival_sasdate#3546, 200), ENSURE_REQUIREMENTS, [id=#6016]\n+- *(14) HashAggregate(keys=[arrival_day#3551, arrival_iso_date#3547, arrival_month#3548, arrival_dayofweek#3549, arrival_weekofyear#3552, arrival_year#3550, arrival_sasdate#3546], functions=[], output=[arrival_day#3551, arrival_iso_date#3547, arrival_month#3548, arrival_dayofweek#3549, arrival_weekofyear#3552, arrival_year#3550, arrival_sasdate#3546])\n   +- *(14) Project [arrdate#2325 AS arrival_sasdate#3546, pythonUDF0#3719 AS arrival_iso_date#3547, date_format(cast(pythonUDF0#3719 as timestamp), M, Some(Asia/Singapore)) AS arrival_month#3548, date_format(cast(pythonUDF0#3719 as timestamp), E, Some(Asia/Singapore)) AS arrival_dayofweek#3549, date_format(cast(pythonUDF0#3719 as timestamp), y, Some(Asia/Singapore)) AS arrival_year#3550, date_format(cast(pythonUDF0#3719 as timestamp), d, Some(Asia/Singapore)) AS arrival_day#3551, date_format(cast(pythonUDF0#3719 as timestamp), w, Some(Asia/Singapore)) AS arrival_weekofyear#3552]\n      +- BatchEvalPython [<lambda>(arrdate#2325)], [pythonUDF0#3719]\n         +- *(13) Project [arrdate#2325]\n            +- SortMergeJoin [upper(port_city#2529), upper(port_state#2530)], [upper(City#16), upper(State_Code#804)], LeftOuter\n               :- *(7) Sort [upper(port_city#2529) ASC NULLS FIRST, upper(port_state#2530) ASC NULLS FIRST], false, 0\n               :  +- Exchange hashpartitioning(upper(port_city#2529), upper(port_state#2530), 200), ENSURE_REQUIREMENTS, [id=#5979]\n               :     +- *(6) Project [arrdate#2325, port_city#2529, port_state#2530]\n               :        +- SortMergeJoin [i94port#2137], [id#2528], LeftOuter\n               :           :- *(3) Sort [i94port#2137 ASC NULLS FIRST], false, 0\n               :           :  +- Exchange hashpartitioning(i94port#2137, 200), ENSURE_REQUIREMENTS, [id=#5966]\n               :           :     +- *(2) HashAggregate(keys=[count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154], functions=[], output=[i94port#2137, arrdate#2325])\n               :           :        +- Exchange hashpartitioning(count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154, 200), ENSURE_REQUIREMENTS, [id=#5962]\n               :           :           +- *(1) HashAggregate(keys=[count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154], functions=[], output=[count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154])\n               :           :              +- *(1) Project [cast(i94res#2136 as int) AS i94res#2324, i94port#2137, cast(arrdate#2138 as int) AS arrdate#2325, cast(i94mode#2139 as int) AS i94mode#2326, cast(depdate#2141 as int) AS depdate#2327, cast(i94bir#2142 as int) AS i94bir#2328, cast(i94visa#2143 as int) AS i94visa#2329, cast(count#2144 as int) AS count#2330, gender#2154, cast(admnum#2157 as bigint) AS admnum#2331L]\n               :           :                 +- *(1) ColumnarToRow\n               :           :                    +- FileScan parquet [i94res#2136,i94port#2137,arrdate#2138,i94mode#2139,depdate#2141,i94bir#2142,i94visa#2143,count#2144,gender#2154,admnum#2157] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/sas_data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i94res:double,i94port:string,arrdate:double,i94mode:double,depdate:double,i94bir:double,i9...\n               :           +- *(5) Sort [id#2528 ASC NULLS FIRST], false, 0\n               :              +- Exchange hashpartitioning(id#2528, 200), ENSURE_REQUIREMENTS, [id=#5971]\n               :                 +- *(4) Filter isnotnull(id#2528)\n               :                    +- *(4) Scan ExistingRDD[id#2528,port_city#2529,port_state#2530]\n               +- *(12) Sort [upper(City#16) ASC NULLS FIRST, upper(State_Code#804) ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(upper(City#16), upper(State_Code#804), 200), ENSURE_REQUIREMENTS, [id=#6003]\n                     +- *(11) Project [City#16, State Code#25 AS State_Code#804]\n                        +- *(11) BroadcastHashJoin [City#16, State Code#25], [City#611, State Code#620], Inner, BuildRight, false\n                           :- *(11) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[City#16, State Code#25])\n                           :  +- Exchange hashpartitioning(Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24, 200), ENSURE_REQUIREMENTS, [id=#5987]\n                           :     +- *(8) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24])\n                           :        +- *(8) Filter (isnotnull(City#16) AND isnotnull(State Code#25))\n                           :           +- FileScan csv [City#16,State#17,Median Age#18,Male Population#19,Female Population#20,Total Population#21,Foreign-born#23,Average Household Size#24,State Code#25] Batched: false, DataFilters: [isnotnull(City#16), isnotnull(State Code#25)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State:string,Median Age:string,Male Population:string,Female Population:string...\n                           +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true], input[1, string, true]),false), [id=#5998]\n                              +- *(10) HashAggregate(keys=[City#611, State Code#620], functions=[], output=[City#611, State Code#620])\n                                 +- Exchange hashpartitioning(City#611, State Code#620, 200), ENSURE_REQUIREMENTS, [id=#5994]\n                                    +- *(9) HashAggregate(keys=[City#611, State Code#620], functions=[], output=[City#611, State Code#620])\n                                       +- *(9) Filter (isnotnull(City#611) AND isnotnull(State Code#620))\n                                          +- FileScan csv [City#611,State Code#620] Batched: false, DataFilters: [isnotnull(City#611), isnotnull(State Code#620)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State Code:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.GeneratedMethodAccessor201.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'w' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:196)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:185)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:109)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:300)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:333)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter(datetimeExpressions.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter$(datetimeExpressions.scala:67)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.getFormatter(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.$anonfun$formatterOption$1(datetimeExpressions.scala:64)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption(datetimeExpressions.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption$(datetimeExpressions.scala:62)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption$lzycompute(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.doGenCode(datetimeExpressions.scala:790)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:163)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$2(basicPhysicalOperators.scala:73)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$1(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1026)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:733)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:47)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 44 more\nCaused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: w, Please use the SQL function EXTRACT instead\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4(DateTimeFormatterHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4$adapted(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.immutable.StringOps.foreach(StringOps.scala:33)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$2(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.convertIncompatiblePattern(DateTimeFormatterHelper.scala:318)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:121)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:59)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:68)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:67)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:108)\n\t... 114 more\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-28c3cd053b63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mi94date_season\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1241.showString.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(arrival_day#3551, arrival_iso_date#3547, arrival_month#3548, arrival_dayofweek#3549, arrival_weekofyear#3552, arrival_year#3550, arrival_sasdate#3546, 200), ENSURE_REQUIREMENTS, [id=#6016]\n+- *(14) HashAggregate(keys=[arrival_day#3551, arrival_iso_date#3547, arrival_month#3548, arrival_dayofweek#3549, arrival_weekofyear#3552, arrival_year#3550, arrival_sasdate#3546], functions=[], output=[arrival_day#3551, arrival_iso_date#3547, arrival_month#3548, arrival_dayofweek#3549, arrival_weekofyear#3552, arrival_year#3550, arrival_sasdate#3546])\n   +- *(14) Project [arrdate#2325 AS arrival_sasdate#3546, pythonUDF0#3719 AS arrival_iso_date#3547, date_format(cast(pythonUDF0#3719 as timestamp), M, Some(Asia/Singapore)) AS arrival_month#3548, date_format(cast(pythonUDF0#3719 as timestamp), E, Some(Asia/Singapore)) AS arrival_dayofweek#3549, date_format(cast(pythonUDF0#3719 as timestamp), y, Some(Asia/Singapore)) AS arrival_year#3550, date_format(cast(pythonUDF0#3719 as timestamp), d, Some(Asia/Singapore)) AS arrival_day#3551, date_format(cast(pythonUDF0#3719 as timestamp), w, Some(Asia/Singapore)) AS arrival_weekofyear#3552]\n      +- BatchEvalPython [<lambda>(arrdate#2325)], [pythonUDF0#3719]\n         +- *(13) Project [arrdate#2325]\n            +- SortMergeJoin [upper(port_city#2529), upper(port_state#2530)], [upper(City#16), upper(State_Code#804)], LeftOuter\n               :- *(7) Sort [upper(port_city#2529) ASC NULLS FIRST, upper(port_state#2530) ASC NULLS FIRST], false, 0\n               :  +- Exchange hashpartitioning(upper(port_city#2529), upper(port_state#2530), 200), ENSURE_REQUIREMENTS, [id=#5979]\n               :     +- *(6) Project [arrdate#2325, port_city#2529, port_state#2530]\n               :        +- SortMergeJoin [i94port#2137], [id#2528], LeftOuter\n               :           :- *(3) Sort [i94port#2137 ASC NULLS FIRST], false, 0\n               :           :  +- Exchange hashpartitioning(i94port#2137, 200), ENSURE_REQUIREMENTS, [id=#5966]\n               :           :     +- *(2) HashAggregate(keys=[count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154], functions=[], output=[i94port#2137, arrdate#2325])\n               :           :        +- Exchange hashpartitioning(count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154, 200), ENSURE_REQUIREMENTS, [id=#5962]\n               :           :           +- *(1) HashAggregate(keys=[count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154], functions=[], output=[count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154])\n               :           :              +- *(1) Project [cast(i94res#2136 as int) AS i94res#2324, i94port#2137, cast(arrdate#2138 as int) AS arrdate#2325, cast(i94mode#2139 as int) AS i94mode#2326, cast(depdate#2141 as int) AS depdate#2327, cast(i94bir#2142 as int) AS i94bir#2328, cast(i94visa#2143 as int) AS i94visa#2329, cast(count#2144 as int) AS count#2330, gender#2154, cast(admnum#2157 as bigint) AS admnum#2331L]\n               :           :                 +- *(1) ColumnarToRow\n               :           :                    +- FileScan parquet [i94res#2136,i94port#2137,arrdate#2138,i94mode#2139,depdate#2141,i94bir#2142,i94visa#2143,count#2144,gender#2154,admnum#2157] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/sas_data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i94res:double,i94port:string,arrdate:double,i94mode:double,depdate:double,i94bir:double,i9...\n               :           +- *(5) Sort [id#2528 ASC NULLS FIRST], false, 0\n               :              +- Exchange hashpartitioning(id#2528, 200), ENSURE_REQUIREMENTS, [id=#5971]\n               :                 +- *(4) Filter isnotnull(id#2528)\n               :                    +- *(4) Scan ExistingRDD[id#2528,port_city#2529,port_state#2530]\n               +- *(12) Sort [upper(City#16) ASC NULLS FIRST, upper(State_Code#804) ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(upper(City#16), upper(State_Code#804), 200), ENSURE_REQUIREMENTS, [id=#6003]\n                     +- *(11) Project [City#16, State Code#25 AS State_Code#804]\n                        +- *(11) BroadcastHashJoin [City#16, State Code#25], [City#611, State Code#620], Inner, BuildRight, false\n                           :- *(11) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[City#16, State Code#25])\n                           :  +- Exchange hashpartitioning(Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24, 200), ENSURE_REQUIREMENTS, [id=#5987]\n                           :     +- *(8) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24])\n                           :        +- *(8) Filter (isnotnull(City#16) AND isnotnull(State Code#25))\n                           :           +- FileScan csv [City#16,State#17,Median Age#18,Male Population#19,Female Population#20,Total Population#21,Foreign-born#23,Average Household Size#24,State Code#25] Batched: false, DataFilters: [isnotnull(City#16), isnotnull(State Code#25)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State:string,Median Age:string,Male Population:string,Female Population:string...\n                           +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true], input[1, string, true]),false), [id=#5998]\n                              +- *(10) HashAggregate(keys=[City#611, State Code#620], functions=[], output=[City#611, State Code#620])\n                                 +- Exchange hashpartitioning(City#611, State Code#620, 200), ENSURE_REQUIREMENTS, [id=#5994]\n                                    +- *(9) HashAggregate(keys=[City#611, State Code#620], functions=[], output=[City#611, State Code#620])\n                                       +- *(9) Filter (isnotnull(City#611) AND isnotnull(State Code#620))\n                                          +- FileScan csv [City#611,State Code#620] Batched: false, DataFilters: [isnotnull(City#611), isnotnull(State Code#620)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State Code:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.GeneratedMethodAccessor201.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'w' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:196)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:185)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:109)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:300)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:333)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter(datetimeExpressions.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter$(datetimeExpressions.scala:67)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.getFormatter(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.$anonfun$formatterOption$1(datetimeExpressions.scala:64)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption(datetimeExpressions.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption$(datetimeExpressions.scala:62)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption$lzycompute(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.doGenCode(datetimeExpressions.scala:790)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:163)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$2(basicPhysicalOperators.scala:73)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$1(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1026)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:733)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:47)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 44 more\nCaused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: w, Please use the SQL function EXTRACT instead\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4(DateTimeFormatterHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4$adapted(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.immutable.StringOps.foreach(StringOps.scala:33)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$2(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.convertIncompatiblePattern(DateTimeFormatterHelper.scala:318)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:121)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:59)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:68)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:67)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:108)\n\t... 114 more\n"
     ]
    }
   ],
   "source": [
    "i94date_season.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1254.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(arrival_day#3551, arrival_iso_date#3547, arrival_month#3548, arrival_dayofweek#3549, arrival_weekofyear#3552, arrival_year#3550, arrival_sasdate#3546, 200), ENSURE_REQUIREMENTS, [id=#6252]\n+- *(14) HashAggregate(keys=[arrival_day#3551, arrival_iso_date#3547, arrival_month#3548, arrival_dayofweek#3549, arrival_weekofyear#3552, arrival_year#3550, arrival_sasdate#3546], functions=[], output=[arrival_day#3551, arrival_iso_date#3547, arrival_month#3548, arrival_dayofweek#3549, arrival_weekofyear#3552, arrival_year#3550, arrival_sasdate#3546])\n   +- *(14) Project [arrdate#2325 AS arrival_sasdate#3546, pythonUDF0#3739 AS arrival_iso_date#3547, date_format(cast(pythonUDF0#3739 as timestamp), M, Some(Asia/Singapore)) AS arrival_month#3548, date_format(cast(pythonUDF0#3739 as timestamp), E, Some(Asia/Singapore)) AS arrival_dayofweek#3549, date_format(cast(pythonUDF0#3739 as timestamp), y, Some(Asia/Singapore)) AS arrival_year#3550, date_format(cast(pythonUDF0#3739 as timestamp), d, Some(Asia/Singapore)) AS arrival_day#3551, date_format(cast(pythonUDF0#3739 as timestamp), w, Some(Asia/Singapore)) AS arrival_weekofyear#3552]\n      +- BatchEvalPython [<lambda>(arrdate#2325)], [pythonUDF0#3739]\n         +- *(13) Project [arrdate#2325]\n            +- SortMergeJoin [upper(port_city#2529), upper(port_state#2530)], [upper(City#16), upper(State_Code#804)], LeftOuter\n               :- *(7) Sort [upper(port_city#2529) ASC NULLS FIRST, upper(port_state#2530) ASC NULLS FIRST], false, 0\n               :  +- Exchange hashpartitioning(upper(port_city#2529), upper(port_state#2530), 200), ENSURE_REQUIREMENTS, [id=#6215]\n               :     +- *(6) Project [arrdate#2325, port_city#2529, port_state#2530]\n               :        +- SortMergeJoin [i94port#2137], [id#2528], LeftOuter\n               :           :- *(3) Sort [i94port#2137 ASC NULLS FIRST], false, 0\n               :           :  +- Exchange hashpartitioning(i94port#2137, 200), ENSURE_REQUIREMENTS, [id=#6202]\n               :           :     +- *(2) HashAggregate(keys=[count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154], functions=[], output=[i94port#2137, arrdate#2325])\n               :           :        +- Exchange hashpartitioning(count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154, 200), ENSURE_REQUIREMENTS, [id=#6198]\n               :           :           +- *(1) HashAggregate(keys=[count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154], functions=[], output=[count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154])\n               :           :              +- *(1) Project [cast(i94res#2136 as int) AS i94res#2324, i94port#2137, cast(arrdate#2138 as int) AS arrdate#2325, cast(i94mode#2139 as int) AS i94mode#2326, cast(depdate#2141 as int) AS depdate#2327, cast(i94bir#2142 as int) AS i94bir#2328, cast(i94visa#2143 as int) AS i94visa#2329, cast(count#2144 as int) AS count#2330, gender#2154, cast(admnum#2157 as bigint) AS admnum#2331L]\n               :           :                 +- *(1) ColumnarToRow\n               :           :                    +- FileScan parquet [i94res#2136,i94port#2137,arrdate#2138,i94mode#2139,depdate#2141,i94bir#2142,i94visa#2143,count#2144,gender#2154,admnum#2157] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/sas_data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i94res:double,i94port:string,arrdate:double,i94mode:double,depdate:double,i94bir:double,i9...\n               :           +- *(5) Sort [id#2528 ASC NULLS FIRST], false, 0\n               :              +- Exchange hashpartitioning(id#2528, 200), ENSURE_REQUIREMENTS, [id=#6207]\n               :                 +- *(4) Filter isnotnull(id#2528)\n               :                    +- *(4) Scan ExistingRDD[id#2528,port_city#2529,port_state#2530]\n               +- *(12) Sort [upper(City#16) ASC NULLS FIRST, upper(State_Code#804) ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(upper(City#16), upper(State_Code#804), 200), ENSURE_REQUIREMENTS, [id=#6239]\n                     +- *(11) Project [City#16, State Code#25 AS State_Code#804]\n                        +- *(11) BroadcastHashJoin [City#16, State Code#25], [City#611, State Code#620], Inner, BuildRight, false\n                           :- *(11) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[City#16, State Code#25])\n                           :  +- Exchange hashpartitioning(Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24, 200), ENSURE_REQUIREMENTS, [id=#6223]\n                           :     +- *(8) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24])\n                           :        +- *(8) Filter (isnotnull(City#16) AND isnotnull(State Code#25))\n                           :           +- FileScan csv [City#16,State#17,Median Age#18,Male Population#19,Female Population#20,Total Population#21,Foreign-born#23,Average Household Size#24,State Code#25] Batched: false, DataFilters: [isnotnull(City#16), isnotnull(State Code#25)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State:string,Median Age:string,Male Population:string,Female Population:string...\n                           +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true], input[1, string, true]),false), [id=#6234]\n                              +- *(10) HashAggregate(keys=[City#611, State Code#620], functions=[], output=[City#611, State Code#620])\n                                 +- Exchange hashpartitioning(City#611, State Code#620, 200), ENSURE_REQUIREMENTS, [id=#6230]\n                                    +- *(9) HashAggregate(keys=[City#611, State Code#620], functions=[], output=[City#611, State Code#620])\n                                       +- *(9) Filter (isnotnull(City#611) AND isnotnull(State Code#620))\n                                          +- FileScan csv [City#611,State Code#620] Batched: false, DataFilters: [isnotnull(City#611), isnotnull(State Code#620)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State Code:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:92)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SortExec.doExecute(SortExec.scala:112)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:184)\n\t... 33 more\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'w' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:196)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:185)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:109)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:300)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:333)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter(datetimeExpressions.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter$(datetimeExpressions.scala:67)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.getFormatter(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.$anonfun$formatterOption$1(datetimeExpressions.scala:64)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption(datetimeExpressions.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption$(datetimeExpressions.scala:62)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption$lzycompute(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.doGenCode(datetimeExpressions.scala:790)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:163)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$2(basicPhysicalOperators.scala:73)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$1(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1026)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:733)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:47)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 63 more\nCaused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: w, Please use the SQL function EXTRACT instead\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4(DateTimeFormatterHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4$adapted(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.immutable.StringOps.foreach(StringOps.scala:33)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$2(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.convertIncompatiblePattern(DateTimeFormatterHelper.scala:318)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:121)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:59)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:68)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:67)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:108)\n\t... 133 more\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-bac54d3875e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save i94date dimension to parquet file partitioned by year and month:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mi94date_season\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrival_year\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arrival_month\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/i94date.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1254.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(arrival_day#3551, arrival_iso_date#3547, arrival_month#3548, arrival_dayofweek#3549, arrival_weekofyear#3552, arrival_year#3550, arrival_sasdate#3546, 200), ENSURE_REQUIREMENTS, [id=#6252]\n+- *(14) HashAggregate(keys=[arrival_day#3551, arrival_iso_date#3547, arrival_month#3548, arrival_dayofweek#3549, arrival_weekofyear#3552, arrival_year#3550, arrival_sasdate#3546], functions=[], output=[arrival_day#3551, arrival_iso_date#3547, arrival_month#3548, arrival_dayofweek#3549, arrival_weekofyear#3552, arrival_year#3550, arrival_sasdate#3546])\n   +- *(14) Project [arrdate#2325 AS arrival_sasdate#3546, pythonUDF0#3739 AS arrival_iso_date#3547, date_format(cast(pythonUDF0#3739 as timestamp), M, Some(Asia/Singapore)) AS arrival_month#3548, date_format(cast(pythonUDF0#3739 as timestamp), E, Some(Asia/Singapore)) AS arrival_dayofweek#3549, date_format(cast(pythonUDF0#3739 as timestamp), y, Some(Asia/Singapore)) AS arrival_year#3550, date_format(cast(pythonUDF0#3739 as timestamp), d, Some(Asia/Singapore)) AS arrival_day#3551, date_format(cast(pythonUDF0#3739 as timestamp), w, Some(Asia/Singapore)) AS arrival_weekofyear#3552]\n      +- BatchEvalPython [<lambda>(arrdate#2325)], [pythonUDF0#3739]\n         +- *(13) Project [arrdate#2325]\n            +- SortMergeJoin [upper(port_city#2529), upper(port_state#2530)], [upper(City#16), upper(State_Code#804)], LeftOuter\n               :- *(7) Sort [upper(port_city#2529) ASC NULLS FIRST, upper(port_state#2530) ASC NULLS FIRST], false, 0\n               :  +- Exchange hashpartitioning(upper(port_city#2529), upper(port_state#2530), 200), ENSURE_REQUIREMENTS, [id=#6215]\n               :     +- *(6) Project [arrdate#2325, port_city#2529, port_state#2530]\n               :        +- SortMergeJoin [i94port#2137], [id#2528], LeftOuter\n               :           :- *(3) Sort [i94port#2137 ASC NULLS FIRST], false, 0\n               :           :  +- Exchange hashpartitioning(i94port#2137, 200), ENSURE_REQUIREMENTS, [id=#6202]\n               :           :     +- *(2) HashAggregate(keys=[count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154], functions=[], output=[i94port#2137, arrdate#2325])\n               :           :        +- Exchange hashpartitioning(count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154, 200), ENSURE_REQUIREMENTS, [id=#6198]\n               :           :           +- *(1) HashAggregate(keys=[count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154], functions=[], output=[count#2330, i94mode#2326, i94bir#2328, i94port#2137, arrdate#2325, admnum#2331L, depdate#2327, i94visa#2329, i94res#2324, gender#2154])\n               :           :              +- *(1) Project [cast(i94res#2136 as int) AS i94res#2324, i94port#2137, cast(arrdate#2138 as int) AS arrdate#2325, cast(i94mode#2139 as int) AS i94mode#2326, cast(depdate#2141 as int) AS depdate#2327, cast(i94bir#2142 as int) AS i94bir#2328, cast(i94visa#2143 as int) AS i94visa#2329, cast(count#2144 as int) AS count#2330, gender#2154, cast(admnum#2157 as bigint) AS admnum#2331L]\n               :           :                 +- *(1) ColumnarToRow\n               :           :                    +- FileScan parquet [i94res#2136,i94port#2137,arrdate#2138,i94mode#2139,depdate#2141,i94bir#2142,i94visa#2143,count#2144,gender#2154,admnum#2157] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/sas_data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<i94res:double,i94port:string,arrdate:double,i94mode:double,depdate:double,i94bir:double,i9...\n               :           +- *(5) Sort [id#2528 ASC NULLS FIRST], false, 0\n               :              +- Exchange hashpartitioning(id#2528, 200), ENSURE_REQUIREMENTS, [id=#6207]\n               :                 +- *(4) Filter isnotnull(id#2528)\n               :                    +- *(4) Scan ExistingRDD[id#2528,port_city#2529,port_state#2530]\n               +- *(12) Sort [upper(City#16) ASC NULLS FIRST, upper(State_Code#804) ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(upper(City#16), upper(State_Code#804), 200), ENSURE_REQUIREMENTS, [id=#6239]\n                     +- *(11) Project [City#16, State Code#25 AS State_Code#804]\n                        +- *(11) BroadcastHashJoin [City#16, State Code#25], [City#611, State Code#620], Inner, BuildRight, false\n                           :- *(11) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[City#16, State Code#25])\n                           :  +- Exchange hashpartitioning(Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24, 200), ENSURE_REQUIREMENTS, [id=#6223]\n                           :     +- *(8) HashAggregate(keys=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24], functions=[], output=[Total Population#21, Female Population#20, City#16, Median Age#18, State Code#25, Foreign-born#23, Male Population#19, State#17, Average Household Size#24])\n                           :        +- *(8) Filter (isnotnull(City#16) AND isnotnull(State Code#25))\n                           :           +- FileScan csv [City#16,State#17,Median Age#18,Male Population#19,Female Population#20,Total Population#21,Foreign-born#23,Average Household Size#24,State Code#25] Batched: false, DataFilters: [isnotnull(City#16), isnotnull(State Code#25)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State:string,Median Age:string,Male Population:string,Female Population:string...\n                           +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true], input[1, string, true]),false), [id=#6234]\n                              +- *(10) HashAggregate(keys=[City#611, State Code#620], functions=[], output=[City#611, State Code#620])\n                                 +- Exchange hashpartitioning(City#611, State Code#620, 200), ENSURE_REQUIREMENTS, [id=#6230]\n                                    +- *(9) HashAggregate(keys=[City#611, State Code#620], functions=[], output=[City#611, State Code#620])\n                                       +- *(9) Filter (isnotnull(City#611) AND isnotnull(State Code#620))\n                                          +- FileScan csv [City#611,State Code#620] Batched: false, DataFilters: [isnotnull(City#611), isnotnull(State Code#620)], Format: CSV, Location: InMemoryFileIndex[file:/home/terry/Desktop/DataEngineering/Capstone/datasets/us-cities-demographi..., PartitionFilters: [], PushedFilters: [IsNotNull(City), IsNotNull(State Code)], ReadSchema: struct<City:string,State Code:string>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:92)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SortExec.doExecute(SortExec.scala:112)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:184)\n\t... 33 more\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'w' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:196)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:185)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:109)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:300)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:333)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter(datetimeExpressions.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter$(datetimeExpressions.scala:67)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.getFormatter(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.$anonfun$formatterOption$1(datetimeExpressions.scala:64)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption(datetimeExpressions.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption$(datetimeExpressions.scala:62)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption$lzycompute(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption(datetimeExpressions.scala:771)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.doGenCode(datetimeExpressions.scala:790)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:163)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$2(basicPhysicalOperators.scala:73)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$1(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1026)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:733)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:47)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 63 more\nCaused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: w, Please use the SQL function EXTRACT instead\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4(DateTimeFormatterHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4$adapted(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.immutable.StringOps.foreach(StringOps.scala:33)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$2(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.convertIncompatiblePattern(DateTimeFormatterHelper.scala:318)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:121)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:59)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:68)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:67)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:108)\n\t... 133 more\n"
     ]
    }
   ],
   "source": [
    "# Save i94date dimension to parquet file partitioned by year and month:\n",
    "i94date_season.write.mode(\"overwrite\").partitionBy(\"arrival_year\", \"arrival_month\").parquet('./data/i94date.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the Data Model\n",
    "\n",
    "## 3.1 Conceptual Data Model\n",
    "## Using the dimensional tables saved as parquet files we can implement them on any columnar database in Star Schema model. \n",
    "## Star Schema model was chosen because it will be easier for Data Analysts and Data Scientists to understand and apply queries with best performance outcomes and flexibility. (Refer to databasediagram1.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.2 Mapping out data pipelines\n",
    "\n",
    "a) Packages needed\n",
    "pandas\n",
    "datetime\n",
    "pyspark.sql -> SparkSession -> \"org.apache.hadoop:hadoop-aws:2.7.0\"\n",
    "pyspark.sql.functions -> first, upper, col, udf, date_format, expr\n",
    "pyspark.sql.types -> StructField, StructType, StringType, LongType, IntegerType\n",
    "\n",
    "b) Create the dimensions (i94port, i94visa, i94res, i94mode) from i94_SAS_Labels_Descriptions.SAS file. \n",
    "*NOTE: Once they're created it does not have to be included in future Data Pipeline schedules because these are essentially master records which do not frequently get added or changed onto the dimension tables.\n",
    "\n",
    "Read US Cities Demo dataset (us-cities-demographics.csv) to form us_spark dataframe\n",
    "\n",
    "Create 'us_race_cnt' dataset from us_spark\n",
    "Drop columns we don't need and drop duplicate rows from us_spark\n",
    "\n",
    "Join us_spark with us_race_cnt to form US data set\n",
    "Change state code column name to state_code and other similar problems to avoid parquet complications. Remove whitespaces!\n",
    "\n",
    "Remove the state column\n",
    "Write transformed US dataset onto parquet file\n",
    "\n",
    "Read i94 non-immigration dataset to form i94_spark dataframe\n",
    "Execute type conversion:  numbers to longtype and integertype\n",
    "Drop duplicate rows\n",
    "Read i94port dimension parquet file so we can use it to join with i94_spark. This will add i94port city and state columns to i94_spark dataframe\n",
    "\n",
    "Drop id column from i94_spark dataframe\n",
    "Join US with i94_spark to get fact table i94non_immigrant_port_entry\n",
    "Add \"iso date format\" column arrival_date inside the i94non_immigrant_port_entry dataframe by using custom function.\n",
    "\n",
    "Create time dimension from i94non_immigrant_port_entry and save to parquet file.\n",
    "Drop arrival_date column from i94non_immigrant_port_entry and save it to parquet file.\n",
    "\n",
    "Add seasons to i94date_seasons dataframe.\n",
    "Save i94date_seasons to parquet file partitioned by year and month.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 4: Run Pipelines to Model the Data\n",
    "4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "\n",
    "Data pipeline is built inside `etl.py`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "us.count EQUALS us_race_cnt.count. PASS\n",
      "us_spark read PASS.\n",
      "i94_spark read PASS\n",
      "i94_spark.count EQUALS i94non_immigrant_port_entry.count. PASS\n"
     ]
    }
   ],
   "source": [
    "### --- Data quality checks here --- ###\n",
    "\n",
    "def check_consistent_data():\n",
    "    if us.count() == us_race_cnt.count():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_usSpark_contents():\n",
    "    if us_spark.count() > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_i94Spark_contents():\n",
    "    if i94_spark.count() > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_i94_data_count():\n",
    "    if i94_spark.count() == i94non_immigrant_port_entry.count():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "test1 = check_consistent_data()\n",
    "if test1 == True:\n",
    "    print('us.count EQUALS us_race_cnt.count. PASS')\n",
    "elif test1 == False:\n",
    "    print('us.count & us_race_cnt have inconsistent data between them. FAIL')\n",
    "\n",
    "test2 = check_usSpark_contents()\n",
    "if test2 == True:\n",
    "    print('us_spark read PASS.')\n",
    "elif test2 == False:\n",
    "    print('Nothing inside us_spark. FAIL')\n",
    "\n",
    "test3 = check_i94Spark_contents()\n",
    "if test3 == True:\n",
    "    print('i94_spark read PASS')\n",
    "elif test3 == False:\n",
    "    print('Nothing inside i94_spark. FAIL')\n",
    "    \n",
    "test4 = check_i94_data_count()\n",
    "if test4 == True:\n",
    "    print('i94_spark.count EQUALS i94non_immigrant_port_entry.count. PASS')\n",
    "elif test4 == False:\n",
    "    print('i94_spark.count & i94non_immigrant_port_entry.count are not equal. FAIL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.3 Data Dictionary\n",
    "Data dictionary is created in file: `4.3_data_dictionary.txt`.\n",
    "Please refer to it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5 Project Write Up\n",
    "Project write-up will be done in `5_project_writeup.txt`.\n",
    "Please refer to it there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Sample Query - \n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "i94visa_pq_file = \"/home/terry/Desktop/DataEngineering/Capstone/data/i94visa.parquet\"\n",
    "df_i94visa = pq.read_table(source=i94visa_pq_file).to_pandas()\n",
    "\n",
    "i94port_pq_file = \"/home/terry/Desktop/DataEngineering/Capstone/data/i94port.parquet\"\n",
    "df_i94port = pq.read_table(source=i94port_pq_file).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ISBURG                         PA\n362  HSB                   HARRISONBURG                         PA\n363  PHI                   PHILADELPHIA                         PA\n364  PIT                      PITTSBURG                         PA\n365  AGU                      AGUADILLA                         PR\n366  BQN          BORINQUEN - AGUADILLO                         PR\n367  JCP      CULEBRA - BENJAMIN RIVERA                         PR\n368  ENS                       ENSENADA                         PR\n369  FAJ                        FAJARDO                         PR\n370  HUM                        HUMACAO                         PR\n371  JOB                          JOBOS                         PR\n372  MAY                       MAYAGUEZ                         PR\n373  PON                          PONCE                         PR\n374  PSE                PONCE-MERCEDITA                         PR\n375  SAJ                       SAN JUAN                         PR\n376  VQS                   VIEQUES-ARPT                         PR\n377  PRO                     PROVIDENCE                         RI\n378  PVD     THEODORE FRANCIS - WARWICK                         RI\n379  CHL                     CHARLESTON                         SC\n380  CAE                       COLUMBIA                   SC #ARPT\n381  GEO                     GEORGETOWN                         SC\n382  GSP                     GREENVILLE                         SC\n383  GRR                          GREER                         SC\n384  MYR                   MYRTLE BEACH                         SC\n385  SPF                    BLACK HILLS              SPEARFISH, SD\n386  HON    HOWES REGIONAL ARPT - HURON                         SD\n387  SAI                         SAIPAN                        SPN\n388  TYS          MC GHEE TYSON - ALCOA                         TN\n389  MEM                        MEMPHIS                         TN\n390  NSV                      NASHVILLE                         TN\n391  TRI                  TRI CITY ARPT                         TN\n392  ADS       ADDISON AIRPORT- ADDISON                         TX\n393  ADT                    AMISTAD DAM                         TX\n394  ANZ                      ANZALDUAS                         TX\n395  AUS                         AUSTIN                         TX\n396  BEA                       BEAUMONT                         TX\n397  BBP                  BIG BEND PARK                   TX (BPS)\n398  SCC             BP SPEC COORD. CTR                         TX\n399  BTC               BP TACTICAL UNIT                         TX\n400  BOA             BRIDGE OF AMERICAS                         TX\n401  BRO                    BROWNSVILLE                         TX\n402  CRP                 CORPUS CHRISTI                         TX\n403  DAL                         DALLAS                         TX\n404  DLR                        DEL RIO                         TX\n405  DNA                          DONNA                         TX\n406  EGP                     EAGLE PASS                         TX\n407  ELP                        EL PASO                         TX\n408  FAB                         FABENS                         TX\n409  FAL                 FALCON HEIGHTS                         TX\n410  FTH                   FORT HANCOCK                         TX\n411  AFW            FORT WORTH ALLIANCE                         TX\n412  FPT                       FREEPORT                         TX\n413  GAL                      GALVESTON                         TX\n414  HLG                      HARLINGEN                         TX\n415  HID                        HIDALGO                         TX\n416  HOU                        HOUSTON                         TX\n417  SGR                     HULL FIELD        SUGAR LAND ARPT, TX\n418  LLB          JUAREZ-LINCOLN BRIDGE                         TX\n419  LCB         LAREDO COLUMBIA BRIDGE                         TX\n420  LRN                   LAREDO NORTH                         TX\n421  LAR                         LAREDO                         TX\n422  LSE                     LOS EBANOS                         TX\n423  IND                     LOS INDIOS                         TX\n424  LOI                     LOS INDIOS                         TX\n425  MRS                          MARFA                   TX (BPS)\n426  MCA                        MCALLEN                         TX\n427  MAF                ODESSA REGIONAL                         TX\n428  PDN                 PASO DEL NORTE                         TX\n429  PBB                   PEACE BRIDGE                         NY\n430  PHR                          PHARR                         TX\n431  PAR                    PORT ARTHUR                         TX\n432  ISB                    PORT ISABEL                         TX\n433  POE                PORT OF EL PASO                         TX\n434  PRE                       PRESIDIO                         TX\n435  PGR                       PROGRESO                         TX\n436  RIO                RIO GRANDE CITY                         TX\n437  ROM                           ROMA                         TX\n438  SNA                    SAN ANTONIO                         TX\n439  SNN                      SANDERSON                         TX\n440  VIB            VETERAN INTL BRIDGE                         TX\n441  YSL                         YSLETA                         TX\n442  CHA               CHARLOTTE AMALIE                         VI\n443  CHR                  CHRISTIANSTED                         VI\n444  CRU                       CRUZ BAY                ST JOHN, VI\n445  FRK                   FREDERIKSTED                         VI\n446  STT                      ST THOMAS                         VI\n447  LGU          CACHE AIRPORT - LOGAN                         UT\n448  SLC                 SALT LAKE CITY                         UT\n449  CHO      ALBEMARLE CHARLOTTESVILLE                         VA\n450  DAA     DAVISON AAF - FAIRFAX CNTY                         VA\n451  HOP                       HOPEWELL                         VA\n452  HEF                       MANASSAS                   VA #ARPT\n453  NWN                        NEWPORT                         VA\n454  NOR                        NORFOLK                         VA\n455  RCM                       RICHMOND                         VA\n456  ABS                 ALBURG SPRINGS                         VT\n457  ABG                         ALBURG                         VT\n458  BEB                    BEEBE PLAIN                         VT\n459  BEE                  BEECHER FALLS                         VT\n460  BRG                     BURLINGTON                         VT\n461  CNA                         CANAAN                         VT\n462  DER                     DERBY LINE                  VT (I-91)\n463  DLV                     DERBY LINE                 VT (RT. 5)\n464  ERC                  EAST RICHFORD                         VT\n465  HIG               HIGHGATE SPRINGS                         VT\n466  MOR                    MORSES LINE                         VT\n467  NPV                        NEWPORT                         VT\n468  NRT                     NORTH TROY                         VT\n469  NRN                         NORTON                         VT\n470  PIV                  PINNACLE ROAD                         VT\n471  RIF                       RICHFORT                         VT\n472  STA                      ST ALBANS                         VT\n473  SWB                        SWANTON        VT (BP - SECTOR HQ)\n474  WBE                 WEST BERKSHIRE                         VT\n475  ABE                       ABERDEEN                         WA\n476  ANA                      ANACORTES                         WA\n477  BEL                     BELLINGHAM                         WA\n478  BLI                     BELLINGHAM           WASHINGTON #INTL\n479  BLA                         BLAINE                         WA\n480  BWA                       BOUNDARY                         WA\n481  CUR                         CURLEW                   WA (BPS)\n482  DVL                       DANVILLE                         WA\n483  EVE                        EVERETT                         WA\n484  FER                          FERRY                         WA\n485  FRI                  FRIDAY HARBOR                         WA\n486  FWA                       FRONTIER                         WA\n487  KLM                         KALAMA                         WA\n488  LAU                        LAURIER                         WA\n489  LON                       LONGVIEW                         WA\n490  MET                 METALINE FALLS                         WA\n491  MWH   MOSES LAKE GRANT COUNTY ARPT                         WA\n492  NEA                       NEAH BAY                         WA\n493  NIG                      NIGHTHAWK                         WA\n494  OLY                        OLYMPIA                         WA\n495  ORO                       OROVILLE                         WA\n496  PWB                          PASCO                         WA\n497  PIR                  POINT ROBERTS                         WA\n498  PNG                   PORT ANGELES                         WA\n499  PTO                  PORT TOWNSEND                         WA\n500  SEA                        SEATTLE                         WA\n501  SPO                        SPOKANE                         WA\n502  SUM                          SUMAS                         WA\n503  TAC                         TACOMA                         WA\n504  PSC             TRI-CITIES - PASCO                         WA\n505  VAN                      VANCOUVER                         WA\n506  AGM                         ALGOMA                         WI\n507  BAY                       BAYFIELD                         WI\n508  GRB                      GREEN BAY                         WI\n509  MNW                      MANITOWOC                         WI\n510  MIL                      MILWAUKEE                         WI\n511  MSN      TRUAX FIELD - DANE COUNTY                         WI\n512  CHS                     CHARLESTON                         WV\n513  CLK                     CLARKSBURG                         WV\n514  BLF                  MERCER COUNTY                         WV\n515  CSP                         CASPER                         WY\n516  XXX           NOT REPORTED/UNKNOWN                        NaN\n517  888      UNIDENTIFED AIR / SEAPORT                        NaN\n518  UNK                    UNKNOWN POE                        NaN\n519  CLG                        CALGARY                     CANADA\n520  EDA                       EDMONTON                     CANADA\n521  YHC                     HAKAI PASS                     CANADA\n522  HAL                        Halifax                 NS, Canada\n523  MON                       MONTREAL                     CANADA\n524  OTT                         OTTAWA                     CANADA\n525  YXE                      SASKATOON                     CANADA\n526  TOR                        TORONTO                     CANADA\n527  VCV                      VANCOUVER                     CANADA\n528  VIC                       VICTORIA                     CANADA\n529  WIN                       WINNIPEG                     CANADA\n530  AMS             AMSTERDAM-SCHIPHOL                NETHERLANDS\n531  ARB                          ARUBA              NETH ANTILLES\n532  BAN                         BANKOK                   THAILAND\n533  BEI                    BEICA #ARPT                   ETHIOPIA\n534  PEK           BEIJING CAPITAL INTL                        PRC\n535  BDA                  KINDLEY FIELD                    BERMUDA\n536  BOG                         BOGOTA  EL DORADO #ARPT, COLOMBIA\n537  EZE                   BUENOS AIRES   MINISTRO PIST, ARGENTINA\n538  CUN                         CANCUN                     MEXICO\n539  CRQ                      CARAVELAS           BA #ARPT, BRAZIL\n540  MVD                       CARRASCO                    URUGUAY\n541  DUB                         DUBLIN                    IRELAND\n542  FOU                 FOUGAMOU #ARPT                      GABON\n543  FBA                       FREEPORT                    BAHAMAS\n544  MTY                GEN M. ESCOBEDO              Monterrey, MX\n545  HMO           GEN PESQUEIRA GARCIA                         MX\n546  GCM                   GRAND CAYMAN              CAYMAN ISLAND\n547  GDL                    GUADALAJARA           MIGUEL HIDAL, MX\n548  HAM                       HAMILTON                    BERMUDA\n549  ICN                         INCHON                SEOUL KOREA\n550  IWA              INVALID - IWAKUNI                      JAPAN\n551  CND                   KOGALNICEANU                    ROMANIA\n552  LAH                    LABUHA ARPT                  INDONESIA\n553  DUR                    LOUIS BOTHA               SOUTH AFRICA\n554  MAL                   MANGOLE ARPT                  INDONESIA\n555  MDE                       MEDELLIN                   COLOMBIA\n556  MEX                    JUAREZ INTL            MEXICO CITY, MX\n557  LHR                      MIDDLESEX                    ENGLAND\n558  NBO                        NAIROBI                      KENYA\n559  NAS                         NASSAU                    BAHAMAS\n560  NCA                   NORTH CAICOS              TURK & CAIMAN\n561  PTY                  OMAR TORRIJOS                     PANAMA\n562  SPV                          PAPUA                 NEW GUINEA\n563  UIO          QUITO (MARISCAL SUCR)                    ECUADOR\n564  RIT                           ROME                      ITALY\n565  SNO             SAKON NAKHON #ARPT                   THAILAND\n566  SLP          SAN LUIS POTOSI #ARPT                     MEXICO\n567  SAN                   SAN SALVADOR                EL SALVADOR\n568  SRO            SANTANA RAMOS #ARPT                   COLOMBIA\n569  GRU                 GUARULHOS INTL          SAO PAULO, BRAZIL\n570  SHA                        SHANNON                    IRELAND\n571  HIL                       SHILLAVO                   ETHIOPIA\n572  TOK                 TOROKINA #ARPT          PAPUA, NEW GUINEA\n573  VER                       VERACRUZ                     MEXICO\n574  LGW                    WEST SUSSEX                    ENGLAND\n575  ZZZ  MEXICO Land (Banco de Mexico)                        NaN\n576  CHN             No PORT Code (CHN)                        NaN\n577  CNC                 CANNON CORNERS                         NY\n578  MAA                      Abu Dhabi                        NaN\n579  AG0                       MAGNOLIA                         AR\n580  BHM                     BAR HARBOR                         ME\n581  BHX                     BIRMINGHAM                         AL\n582  CAK                          AKRON                         OH\n583  FOK                 SUFFOLK COUNTY                         NY\n584  LND                         LANDER                         WY\n585  MAR                          MARFA                         TX\n586  MLI                         MOLINE                         IL\n587  RIV                      RIVERSIDE                         CA\n588  RME                           ROME                         NY\n589  VNY                       VAN NUYS                         CA\n590  YUM                           YUMA                         AZ\n591  FRG          Collapsed (FOK) 06/15                        NaN\n592  HRL          Collapsed (HLG) 06/15                        NaN\n593  ISP          Collapsed (FOK) 06/15                        NaN\n594  JSJ          Collapsed (SAJ) 06/15                        NaN\n595  BUS          Collapsed (BUF) 06/15                        NaN\n596  IAG          Collapsed (NIA) 06/15                        NaN\n597  PHN          Collapsed (PHU) 06/15                        NaN\n598  STN          Collapsed (STR) 06/15                        NaN\n599  VMB          Collapsed (VNB) 06/15                        NaN\n600  T01          Collapsed (SEA) 06/15                        NaN\n601  PHF             No PORT Code (PHF)                        NaN\n602  DRV             No PORT Code (DRV)                        NaN\n603  FTB             No PORT Code (FTB)                        NaN\n604  GAC             No PORT Code (GAC)                        NaN\n605  GMT             No PORT Code (GMT)                        NaN\n606  JFA             No PORT Code (JFA)                        NaN\n607  JMZ             No PORT Code (JMZ)                        NaN\n608  NC8             No PORT Code (NC8)                        NaN\n609  NYL             No PORT Code (NYL)                        NaN\n610  OAI             No PORT Code (OAI)                        NaN\n611  PCW             No PORT Code (PCW)                        NaN\n612  WA5             No PORT Code (WAS)                        NaN\n613  WTR             No PORT Code (WTR)                        NaN\n614  X96             No PORT Code (X96)                        NaN\n615  XNA             No PORT Code (XNA)                        NaN\n616  YGF             No PORT Code (YGF)                        NaN\n617  5T6             No PORT Code (5T6)                        NaN\n618  060              No PORT Code (60)                        NaN\n619  SP0             No PORT Code (SP0)                        NaN\n620  W55             No PORT Code (W55)                        NaN\n621  X44             No PORT Code (X44)                        NaN\n622  AUH             No PORT Code (AUH)                        NaN\n623  RYY             No PORT Code (RYY)                        NaN\n624  SUS             No PORT Code (SUS)                        NaN\n625  74S             No PORT Code (74S)                        NaN\n626  ATW             No PORT Code (ATW)                        NaN\n627  CPX             No PORT Code (CPX)                        NaN\n628  MTH             No PORT Code (MTH)                        NaN\n629  PFN             No PORT Code (PFN)                        NaN\n630  SCH             No PORT Code (SCH)                        NaN\n631  ASI             No PORT Code (ASI)                        NaN\n632  BKF             No PORT Code (BKF)                        NaN\n633  DAY             No PORT Code (DAY)                        NaN\n634  Y62             No PORT Code (Y62)                        NaN\n635   AG              No PORT Code (AG)                        NaN\n636  BCM             No PORT Code (BCM)                        NaN\n637  DEC             No PORT Code (DEC)                        NaN\n638  PLB             No PORT Code (PLB)                        NaN\n639  CXO             No PORT Code (CXO)                        NaN\n640  JBQ             No PORT Code (JBQ)                        NaN\n641  JIG             No PORT Code (JIG)                        NaN\n642  OGS             No PORT Code (OGS)                        NaN\n643  TIW             No PORT Code (TIW)                        NaN\n644  OTS             No PORT Code (OTS)                        NaN\n645  AMT             No PORT Code (AMT)                        NaN\n646  EGE             No PORT Code (EGE)                        NaN\n647  GPI             No PORT Code (GPI)                        NaN\n648  NGL             No PORT Code (NGL)                        NaN\n649  OLM             No PORT Code (OLM)                        NaN\n650  .GA             No PORT Code (.GA)                        NaN\n651  CLX             No PORT Code (CLX)                        NaN\n652  CP               No PORT Code (CP)                        NaN\n653  FSC             No PORT Code (FSC)                        NaN\n654   NK              No PORT Code (NK)                        NaN\n655  ADU             No PORT Code (ADU)                        NaN\n656  AKT             No PORT Code (AKT)                        NaN\n657  LIT             No PORT Code (LIT)                        NaN\n658  A2A             No PORT Code (A2A)                        NaN\n659  OSN             No PORT Code (OSN)                        NaN\n"
     ]
    }
   ],
   "source": [
    "print(df_i94port.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}